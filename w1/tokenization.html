
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>3. Tokenization &#8212; New Languages for NLP Building Linguistic Diversity in the Digital Humanities</title>
    
  <link rel="stylesheet" href="../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.e7340bb3dbd8dde6db86f25597f54a1b.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.7d483ff0a819d6edff12ce0b1ead3928.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="5. Annotation with INCEpTION" href="annotation.html" />
    <link rel="prev" title="2. Introduction to Linguistic Data" href="intro-to-linguistic-data.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">New Languages for NLP Building Linguistic Diversity in the Digital Humanities</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   New Languages for NLP: Building Linguistic Diversity in the Digital Humanities
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Preparation
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../content.html">
   1. Content in Jupyter Book
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../core-technologies.html">
   2. Technology and Tools Reference Guide (ready by April 17, yes you heard me right April 17, that’s one month from now)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../markdown.html">
   3. Markdown Files
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Workshop I Annotation and Linguistic Data
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="overview.html">
   1. Welcome to Workshop I
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="intro-to-linguistic-data.html">
   2. Introduction to Linguistic Data
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   3. Tokenization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="#building-a-new-language-tokenizer">
   4. Building A New Language Tokenizer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="annotation.html">
   5. Annotation with INCEpTION
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/w1/tokenization.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->


            <!-- Full screen (wrap in <a> to have style consistency -->
            <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                    data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                    title="Fullscreen mode"><i
                        class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i>
            Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   3. Tokenization
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#natural-language-processing-tokenization">
     3.1. Natural language processing &amp; Tokenization
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#adding-new-exceptions-for-your-language">
     3.2. Adding new exceptions for your language
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#specific-exceptions">
     3.3. Specific Exceptions
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#normalized-forms-for-variations-and-abbreviations">
       3.3.1. Normalized forms for variations and abbreviations
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#separate-a-word-into-two-tokens">
     3.4. Separate a word into two tokens
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rule-based-exceptions">
     3.5. Rule-based exceptions
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#extending-defaults">
     3.6. Extending defaults
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#prefixes-in-action">
       3.6.1. Prefixes in action
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#infix">
       3.6.2. Infix
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#suffix">
       3.6.3. Suffix
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#building-a-new-language-tokenizer">
   4. Building A New Language Tokenizer
  </a>
 </li>
</ul>

        </nav>
        
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="tokenization">
<h1><span class="section-number">3. </span>Tokenization<a class="headerlink" href="#tokenization" title="Permalink to this headline">¶</a></h1>
<p>From your computer’s perspective, text is nothing more than a sequence of characters. If you ask Python to iterate over a snippet of text, you’ll see that it returns just one letter at a time. Note that the index starts at 0, not 1 and that spaces are part of the sequence.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;Siberia has many rivers.&quot;</span>
<span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">char</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">char</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">0</span>  <span class="n">S</span>
<span class="mi">1</span>  <span class="n">i</span>
<span class="mi">2</span>  <span class="n">b</span>
<span class="mi">3</span>  <span class="n">e</span>
<span class="mi">4</span>  <span class="n">r</span>
<span class="mi">5</span>  <span class="n">i</span>
<span class="mi">6</span>  <span class="n">a</span>
<span class="mi">7</span>  
<span class="mi">8</span>  <span class="n">h</span>
<span class="mi">9</span>  <span class="n">a</span>
<span class="mi">10</span> <span class="n">s</span>
<span class="mi">11</span>  
<span class="mi">12</span> <span class="n">m</span>
<span class="mi">13</span> <span class="n">a</span>
<span class="mi">14</span> <span class="n">n</span>
<span class="mi">15</span> <span class="n">y</span>
<span class="mi">16</span>  
<span class="mi">17</span> <span class="n">r</span>
<span class="mi">18</span> <span class="n">i</span>
<span class="mi">19</span> <span class="n">v</span>
<span class="mi">20</span> <span class="n">e</span>
<span class="mi">21</span> <span class="n">r</span>
<span class="mi">22</span> <span class="n">s</span>
<span class="mi">23</span> <span class="o">.</span>
</pre></div>
</div>
<p>When we ask Python to find a word, say “rivers”, in a larger text, it is actually searching for a lower-case “r” followed by “i” “v” and so on. It returns a match only if it finds exactly the right letters in the right order.  When it makes a match, Python’s .find() function will return the location of the first character in the sequence. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;Siberia has many rivers.&quot;</span>
<span class="n">text</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="s2">&quot;rivers&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">17</span>
</pre></div>
</div>
<p>Keep in mind that computers are very precise and picky.  Any messiness in the text will cause the word to be missed, so <code class="docutils literal notranslate"><span class="pre">text.find(&quot;Rivers&quot;)</span></code> returns -1, which means that the sequence could not be found. You can also accidentally match characters that are part of the sequence, but not part of a word.  Try <code class="docutils literal notranslate"><span class="pre">text.find(&quot;y</span> <span class="pre">riv&quot;)</span></code>.  You get 15 as the answer because that is the beginning of the “y riv” sequence, which is present in the text, but isn’t a thing that you’d normally want to find.</p>
<div class="section" id="natural-language-processing-tokenization">
<h2><span class="section-number">3.1. </span>Natural language processing &amp; Tokenization<a class="headerlink" href="#natural-language-processing-tokenization" title="Permalink to this headline">¶</a></h2>
<p>While pure Python is sufficient for many tasks, natural language processing (NLP) libraries allow us to work computationally with the text as language. NLP reveals a whole host of linguistic attributes of the text that can be used for analysis.  For example, the machine will know if a word is a noun or a verb with part of speech tagging.  We can find the direct object of a verb to determine who is speaking and the subject of that speech.  NLP gives your programs an instant boost of information that opens new forms of analysis.</p>
<p>Our first NLP task is tokenization. This is where our text is split into meaningful parts; usually word tokens. The sentence, “Siberia has many rivers.” can be split into the tokens: <code class="docutils literal notranslate"><span class="pre">&lt;Siberia&gt;&lt;has&gt;&lt;many&gt;&lt;rivers&gt;&lt;.&gt;</span></code>  Note that the ending punctuation is now distinct from the word rivers. The rules for tokenization depend on the language your are using. For English and other languages with spaces between words, you often get good results simply by splitting the tokens on spaces. However, a host of rules are also needed to separate punctuation from a token, to split and normalize words (ex. “Let’s” &gt; Let us) as well as specific exceptions that don’t follow regular patterns. The <a class="reference external" href="https://spacy.io/usage/linguistic-features/#tokenization">spaCy documentation</a> is really excellent on this topic and I recommend that you start there.</p>
<p>spaCy’s tokenization rules begins splitting tokens on spaces. It’s nearly identical what you’d get from <code class="docutils literal notranslate"><span class="pre">&quot;Siberia</span> <span class="pre">has</span> <span class="pre">many</span> <span class="pre">rivers.&quot;.split()</span></code>, which is <code class="docutils literal notranslate"><span class="pre">['Siberia','has','many','rivers.']</span></code>  Keep a close eye on the period in this sentence.  Once again, Python had trouble identifying the period as a distinct token. Once the text is split on the spaces, spaCy applies as series of checks.</p>
<ul class="simple">
<li><p>Exceptions: This is a list of specific patterns to look for. For example, “1 am”, will become <code class="docutils literal notranslate"><span class="pre">&lt;1&gt;&lt;a.m.&gt;</span></code> The tokenizer not only notices that ‘am’ isn’t the verb <code class="docutils literal notranslate"><span class="pre">to</span> <span class="pre">be</span></code>, but also that the formatting is ambiguous. If we turn all AM, am, and a.m into a.m. then we have a common unit for analysis. This is especially important when you are interested in word frequencies in a text.</p></li>
</ul>
<p>The exceptions for your language are most often found in <code class="docutils literal notranslate"><span class="pre">spacy/lang</span></code> directory in a <code class="docutils literal notranslate"><span class="pre">tokenizer_exceptions.py</span></code> file. For example, here are the exceptions for English to handle shortened forms of ‘because’ such as ‘cause. These exception prevent the tokenizer from splitting off the <code class="docutils literal notranslate"><span class="pre">'</span></code> from <code class="docutils literal notranslate"><span class="pre">coz</span></code>.</p>
<p><strong><a class="reference external" href="https://github.com/explosion/spaCy/blob/34e13c1161f7d42b961026b12d2eb3d3165bae27/spacy/lang/en/tokenizer_exceptions.py#L392">tokenizer_exceptions.py</a></strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="n">ORTH</span><span class="p">:</span> <span class="s2">&quot;&#39;Cause&quot;</span><span class="p">,</span> <span class="n">NORM</span><span class="p">:</span> <span class="s2">&quot;because&quot;</span><span class="p">},</span>
<span class="p">{</span><span class="n">ORTH</span><span class="p">:</span> <span class="s2">&quot;&#39;cause&quot;</span><span class="p">,</span> <span class="n">NORM</span><span class="p">:</span> <span class="s2">&quot;because&quot;</span><span class="p">},</span>
<span class="p">{</span><span class="n">ORTH</span><span class="p">:</span> <span class="s2">&quot;&#39;cos&quot;</span><span class="p">,</span> <span class="n">NORM</span><span class="p">:</span> <span class="s2">&quot;because&quot;</span><span class="p">},</span>
<span class="p">{</span><span class="n">ORTH</span><span class="p">:</span> <span class="s2">&quot;&#39;Cos&quot;</span><span class="p">,</span> <span class="n">NORM</span><span class="p">:</span> <span class="s2">&quot;because&quot;</span><span class="p">},</span>
<span class="p">{</span><span class="n">ORTH</span><span class="p">:</span> <span class="s2">&quot;&#39;coz&quot;</span><span class="p">,</span> <span class="n">NORM</span><span class="p">:</span> <span class="s2">&quot;because&quot;</span><span class="p">},</span>
<span class="p">{</span><span class="n">ORTH</span><span class="p">:</span> <span class="s2">&quot;&#39;Coz&quot;</span><span class="p">,</span> <span class="n">NORM</span><span class="p">:</span> <span class="s2">&quot;because&quot;</span><span class="p">},</span>
<span class="p">{</span><span class="n">ORTH</span><span class="p">:</span> <span class="s2">&quot;&#39;cuz&quot;</span><span class="p">,</span> <span class="n">NORM</span><span class="p">:</span> <span class="s2">&quot;because&quot;</span><span class="p">},</span>
<span class="p">{</span><span class="n">ORTH</span><span class="p">:</span> <span class="s2">&quot;&#39;Cuz&quot;</span><span class="p">,</span> <span class="n">NORM</span><span class="p">:</span> <span class="s2">&quot;because&quot;</span><span class="p">},</span>
</pre></div>
</div>
<p>Note that the spaCy developers have accounted for the most common variations of ‘because’ and deliberately decided to incorporate slang and idomatic usage. They have added a normalized form (NORM) of <code class="docutils literal notranslate"><span class="pre">because</span></code>. If we’re interested in word frequencies and not variation, this can be a very useful. This is available to you as <code class="docutils literal notranslate"><span class="pre">token.norm_</span></code>.</p>
<p>If you look at the <code class="docutils literal notranslate"><span class="pre">tokenizer_exceptions.py</span></code> files for the existing languages, you’ll see a wide range of exceptions and ways of writing the rules. For the sake of simplicity, we provide a simple way to add exceptions for your language.</p>
</div>
<div class="section" id="adding-new-exceptions-for-your-language">
<h2><span class="section-number">3.2. </span>Adding new exceptions for your language<a class="headerlink" href="#adding-new-exceptions-for-your-language" title="Permalink to this headline">¶</a></h2>
<p>spaCy comes with a lot of opinions and defaults right from the beginning.  In most cases, this will save you time. You can find the default tokenizer exceptions by importing them.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">spacy.lang.tokenizer_exceptions</span> <span class="kn">import</span> <span class="n">BASE_EXCEPTIONS</span>
</pre></div>
</div>
<p>You’ll find that <code class="docutils literal notranslate"><span class="pre">BASE_EXCEPTIONS</span></code> is a Python dictionary.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">...</span>
<span class="s1">&#39;C++&#39;</span><span class="p">:</span> <span class="p">[{</span><span class="mi">65</span><span class="p">:</span> <span class="s1">&#39;C++&#39;</span><span class="p">}],</span>
 <span class="s1">&#39;a.&#39;</span><span class="p">:</span> <span class="p">[{</span><span class="mi">65</span><span class="p">:</span> <span class="s1">&#39;a.&#39;</span><span class="p">}],</span>
 <span class="s1">&#39;b.&#39;</span><span class="p">:</span> <span class="p">[{</span><span class="mi">65</span><span class="p">:</span> <span class="s1">&#39;b.&#39;</span><span class="p">}],</span>
 <span class="s1">&#39;c.&#39;</span><span class="p">:</span> <span class="p">[{</span><span class="mi">65</span><span class="p">:</span> <span class="s1">&#39;c.&#39;</span><span class="p">}],</span>
 <span class="s1">&#39;d.&#39;</span><span class="p">:</span> <span class="p">[{</span><span class="mi">65</span><span class="p">:</span> <span class="s1">&#39;d.&#39;</span><span class="p">}],</span>
 <span class="s1">&#39;(ಠ_ಠ)&#39;</span><span class="p">:</span> <span class="p">[{</span><span class="mi">65</span><span class="p">:</span> <span class="s1">&#39;(ಠ_ಠ)&#39;</span><span class="p">}],</span>
 <span class="s1">&#39;(&gt;_&lt;)&#39;</span><span class="p">:</span> <span class="p">[{</span><span class="mi">65</span><span class="p">:</span> <span class="s1">&#39;(&gt;_&lt;)&#39;</span><span class="p">}],</span>
 <span class="o">...</span> 
</pre></div>
</div>
<p>spaCy also comes with a nice utility function that lets you add new exceptions to the defaults: <code class="docutils literal notranslate"><span class="pre">update_exc()</span></code>.</p>
<p>For clarity, we will refer to two types of exceptions.  The first are specific, or one-time, exceptions.  These define a very specific pattern for spaCy to look for. If it finds a match, it will apply specific tokenization rules to it. In the example above, <code class="docutils literal notranslate"><span class="pre">'Cuz</span></code> would normally be split into <code class="docutils literal notranslate"><span class="pre">&lt;'&gt;&lt;Cuz&gt;</span></code> because the spaCy defaults would treat <code class="docutils literal notranslate"><span class="pre">'</span></code> as a prefix. To prevent this, we can add a specific exception in <code class="docutils literal notranslate"><span class="pre">tokenizer_exceptions.py</span></code></p>
<p>Rule-based exceptions look for more general patterns. For the example above, we could add an exception for any time we find <code class="docutils literal notranslate"><span class="pre">'</span></code> followed by the letter c. This would be much more flexible and catch more variations on the form. Instead of 8 specific rules, we’d have one pattern.  But be careful, our rule-based pattern would also apply to <code class="docutils literal notranslate"><span class="pre">'cuse</span> <span class="pre">me!</span></code> which is a shortened form of <code class="docutils literal notranslate"><span class="pre">excuse</span> <span class="pre">me!</span></code> That might be a good thing, it might not.</p>
<p>The lesson here is that it’s up to you when to use specific exceptions and when to use rule-based exceptions.</p>
</div>
<div class="section" id="specific-exceptions">
<h2><span class="section-number">3.3. </span>Specific Exceptions<a class="headerlink" href="#specific-exceptions" title="Permalink to this headline">¶</a></h2>
<p>To add a new exception, pass a dictionary with a key with the string to match and a list with instructions on how to transform it.</p>
<p>For example:
“BIG YIKES” would normally be split into two tokens <code class="docutils literal notranslate"><span class="pre">&lt;BIG&gt;&lt;YIKES&gt;</span></code>. To prevent this from happening, we can create an exception.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">spacy.symbols</span> <span class="kn">import</span> <span class="n">ORTH</span>
<span class="kn">from</span> <span class="nn">spacy.util</span> <span class="kn">import</span> <span class="n">update_exc</span>

<span class="n">yikes</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;BIG YIKES&#39;</span><span class="p">:[{</span><span class="n">ORTH</span><span class="p">:</span> <span class="s1">&#39;BIG YIKES&#39;</span><span class="p">}]}</span>
<span class="n">TOKENIZER_EXCEPTIONS</span> <span class="o">=</span> <span class="n">update_exc</span><span class="p">(</span><span class="n">BASE_EXCEPTIONS</span><span class="p">,</span> <span class="n">yikes</span><span class="p">)</span>


</pre></div>
</div>
<p>Let’s test to confirm that our the lemmatizer is acting as we’d expect.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">spacy.lang.en</span> <span class="kn">import</span> <span class="n">English</span>
<span class="kn">from</span> <span class="nn">spacy.lang.tokenizer_exceptions</span> <span class="kn">import</span> <span class="n">BASE_EXCEPTIONS</span>

<span class="c1">#Load the basic English language object</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">English</span><span class="p">()</span> 

<span class="c1">#Here&#39;s our new exception</span>
<span class="n">yikes</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;BIG YIKES&#39;</span><span class="p">:[{</span><span class="n">ORTH</span><span class="p">:</span> <span class="s1">&#39;BIG YIKES&#39;</span><span class="p">}]}</span>

<span class="c1">#Update the default tokenizer with our tokenizer exception</span>
<span class="n">nlp</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">rules</span> <span class="o">=</span> <span class="n">update_exc</span><span class="p">(</span><span class="n">BASE_EXCEPTIONS</span><span class="p">,</span> <span class="n">yikes</span><span class="p">)</span>

<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;Yikes! BIG YIKES!&quot;</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">doc</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">text</span> <span class="o">==</span> <span class="s2">&quot;BIG YIKES&quot;</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>[ t for t in doc]
[Yikes, !, BIG YIKES, !]
</pre></div>
</div>
<p>That’s exciting! We’ve made a change to the tokenization rules and it worked. Just keep in mind that exceptions are very specific.  If we have <code class="docutils literal notranslate"><span class="pre">&quot;Yikes!</span> <span class="pre">BIG</span> <span class="pre">Yikes!&quot;</span></code>, we get “BIG” and “Yikes” as separate tokens because “Yikes” isn’t all in caps. Yes, it’s that picky. When adding exceptions, you’ll want to add rules for all of the variations that your model is likely to encounter.</p>
<p>To build on our momentum, let’s discuss several other common types of tokenizer exceptions.</p>
<div class="section" id="normalized-forms-for-variations-and-abbreviations">
<h3><span class="section-number">3.3.1. </span>Normalized forms for variations and abbreviations<a class="headerlink" href="#normalized-forms-for-variations-and-abbreviations" title="Permalink to this headline">¶</a></h3>
<p>If you look at the <code class="docutils literal notranslate"><span class="pre">tokenizer_exceptions.py</span></code> files in the spacy/langs directory you’ll see that the most common use of exceptions is add a normalized form to slang, misspellings and common abbreviations for words.</p>
<p>For example, “I luv this!”  We want spaCy to recognize that “luv” is a derivative of “love.”</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">luv</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;luv&quot;</span><span class="p">:[{</span><span class="n">ORTH</span><span class="p">:</span><span class="s1">&#39;luv&#39;</span><span class="p">,</span><span class="n">NORM</span><span class="p">:</span> <span class="s1">&#39;love&#39;</span><span class="p">}]}</span>

<span class="n">nlp</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">rules</span> <span class="o">=</span> <span class="n">update_exc</span><span class="p">(</span><span class="n">BASE_EXCEPTIONS</span><span class="p">,</span> <span class="n">luv</span><span class="p">)</span>

<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s1">&#39;I luv this!&#39;</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">doc</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">norm_</span> <span class="o">==</span> <span class="s1">&#39;love&#39;</span>
</pre></div>
</div>
<p>Challenge:
Add rules to the tokenizer so that this sentence “MAH TOKENIZR LOVEZ DIS SENTENCE” returns: [“My”,”tokenizer”,”loves”,”this”,”sentence”] <a class="reference external" href="https://speaklolcat.com/">or make your own!</a></p>
<p>solution:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">exceptions</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;MAH&quot;</span><span class="p">:[{</span><span class="n">ORTH</span><span class="p">:</span><span class="s1">&#39;MAH&#39;</span><span class="p">,</span><span class="n">NORM</span><span class="p">:</span> <span class="s1">&#39;My&#39;</span><span class="p">}]},</span>
    <span class="p">{</span><span class="s2">&quot;TOKENIZR&quot;</span><span class="p">:[{</span><span class="n">ORTH</span><span class="p">:</span><span class="s1">&#39;TOKENIZR&#39;</span><span class="p">,</span><span class="n">NORM</span><span class="p">:</span> <span class="s1">&#39;tokenizer&#39;</span><span class="p">}]},</span>
    <span class="p">{</span><span class="s2">&quot;LOVEZ&quot;</span><span class="p">:[{</span><span class="n">ORTH</span><span class="p">:</span><span class="s1">&#39;LOVEZ&#39;</span><span class="p">,</span><span class="n">NORM</span><span class="p">:</span> <span class="s1">&#39;loves&#39;</span><span class="p">}]},</span>
    <span class="p">{</span><span class="s2">&quot;DIS&quot;</span><span class="p">:[{</span><span class="n">ORTH</span><span class="p">:</span><span class="s1">&#39;DIS&#39;</span><span class="p">,</span><span class="n">NORM</span><span class="p">:</span> <span class="s1">&#39;this&#39;</span><span class="p">}]},</span>
    <span class="p">{</span><span class="s2">&quot;SENTENCE&quot;</span><span class="p">:[{</span><span class="n">ORTH</span><span class="p">:</span><span class="s1">&#39;SENTENCE&#39;</span><span class="p">,</span><span class="n">NORM</span><span class="p">:</span> <span class="s1">&#39;sentence&#39;</span><span class="p">}]},</span>
<span class="p">]</span>

<span class="n">TOKENIZER_EXCEPTIONS</span> <span class="o">=</span> <span class="n">update_exc</span><span class="p">(</span><span class="n">BASE_EXCEPTIONS</span><span class="p">,</span> <span class="o">*</span><span class="n">exceptions</span><span class="p">)</span>

<span class="n">nlp</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">rules</span> <span class="o">=</span> <span class="n">TOKENIZER_EXCEPTIONS</span>

<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s1">&#39;MAH TOKENIZR LOVEZ DIS SENTENCE&#39;</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">[</span><span class="n">token</span><span class="o">.</span><span class="n">norm_</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">]</span> <span class="o">==</span> <span class="p">[</span><span class="s1">&#39;My&#39;</span><span class="p">,</span> <span class="s1">&#39;tokenizer&#39;</span><span class="p">,</span> <span class="s1">&#39;loves&#39;</span><span class="p">,</span> <span class="s1">&#39;this&#39;</span><span class="p">,</span> <span class="s1">&#39;sentence&#39;</span><span class="p">]</span>

</pre></div>
</div>
</div>
</div>
<div class="section" id="separate-a-word-into-two-tokens">
<h2><span class="section-number">3.4. </span>Separate a word into two tokens<a class="headerlink" href="#separate-a-word-into-two-tokens" title="Permalink to this headline">¶</a></h2>
<p>It’s very common to have words that should be split into separate tokens, but there isn’t a regular infix that will make the cut.  Here we need an exception. As an example, let’s explore Kummerspeck (‘grief bacon’) the German name for weight gain from emotional eating.</p>
<p>Here we can use exception’s list to split the word into parts and detail how to handle each of the new tokens: <code class="docutils literal notranslate"><span class="pre">'matchword':[{match}{word}]</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">grief_bacon</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;Kummerspeck&#39;</span><span class="p">:[{</span><span class="n">ORTH</span><span class="p">:</span><span class="s2">&quot;Kummer&quot;</span><span class="p">},{</span><span class="n">ORTH</span><span class="p">:</span><span class="s2">&quot;speck&quot;</span><span class="p">}]}</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">spacy.lang.de</span> <span class="kn">import</span> <span class="n">German</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">German</span><span class="p">()</span>
<span class="n">nlp</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">rules</span> <span class="o">=</span> <span class="n">update_exc</span><span class="p">(</span><span class="n">BASE_EXCEPTIONS</span><span class="p">,</span> <span class="n">grief_bacon</span><span class="p">)</span>

<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;Das Problem ist nur, dass das Gewicht, das McCoy sich anfuttert, nicht nur beruflich bedingt ist, sondern mindestens zur Hälfte aus Kummerspeck besteht.&quot;</span><span class="p">)</span>

<span class="k">assert</span> <span class="n">doc</span><span class="p">[</span><span class="mi">25</span><span class="p">]</span><span class="o">.</span><span class="n">text</span> <span class="o">==</span> <span class="s1">&#39;Kummer&#39;</span> <span class="ow">and</span> <span class="n">doc</span><span class="p">[</span><span class="mi">26</span><span class="p">]</span><span class="o">.</span><span class="n">text</span> <span class="o">==</span> <span class="s1">&#39;speck&#39;</span>
</pre></div>
</div>
<p>The three use cases covered above are the most common types of tokenizer exceptions: adding normalizations, combining words, splitting words. Exceptions can be extremely useful for one-time problems for which there just aren’t any rules or common patterns. However, it’s simply impractical to handle everything with specific exceptions. The next section cover ways that you can handle common patterns and rule-based exceptions.</p>
</div>
<div class="section" id="rule-based-exceptions">
<h2><span class="section-number">3.5. </span>Rule-based exceptions<a class="headerlink" href="#rule-based-exceptions" title="Permalink to this headline">¶</a></h2>
<p>To handle regular patterns, spaCy has three kinds of rule-based exceptions. They are:</p>
<ul class="simple">
<li><p>Prefix: A section at the start of a word that should be separated into its own token.</p></li>
<li><p>Infix: A section in the middle of a word that should be separated into its own token.</p></li>
<li><p>Suffix: A section at the end of a word that should be separated into its own token.</p></li>
</ul>
<p>spaCy comes with default rule-based exceptions that can often be found in the language’s <code class="docutils literal notranslate"><span class="pre">punctuations.py</span></code> file. Additionally, there is a <code class="docutils literal notranslate"><span class="pre">spacy/lang/punctuation.py</span></code> file that has base <code class="docutils literal notranslate"><span class="pre">TOKENIZER_PREFIXES</span></code>, <code class="docutils literal notranslate"><span class="pre">TOKENIZER_SUFFIXES</span></code>, and <code class="docutils literal notranslate"><span class="pre">TOKENIZER_INFIXES</span></code>; <a class="reference external" href="https://github.com/explosion/spaCy/blob/master/spacy/lang/punctuation.py">see here</a>. These defaults are lists of exception patterns. If you look at the files, you’ll see some variety.  Some are just a string such as “<span class="math notranslate nohighlight">\(&quot; as a prefix for dollars. '\)</span>100’ will be split into <code class="docutils literal notranslate"><span class="pre">&lt;$&gt;&lt;100&gt;</span></code>. Another approach is to work by charachter type.  For example we could just add <code class="docutils literal notranslate"><span class="pre">LIST_CURRENCY</span></code> as a prefix.  Now spaCy will separate all of the listed currency symbols for you (‘$’, ‘£’, ‘€’, ‘¥’, ‘฿’…). There’s a large menu of charachter types available to you in <a class="reference external" href="https://github.com/explosion/spaCy/blob/master/spacy/lang/char_classes.py">char_classes.py</a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">spacy.lang.char_classes</span> <span class="kn">import</span> <span class="n">LIST_CURRENCY</span>

<span class="n">TOKENIZER_PREFIXES</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">LIST_CURRENCY</span>
<span class="p">)</span>
</pre></div>
</div>
<p>A third approach uses regular expressions. Regex is a serious pain in the butt.  There are regex masters out there, but most people Google helplessly until they get it to work.  A very helpful resource is <a class="reference external" href="https://regex101.com/">regex101</a>.  This is a website that let’s you build a regular expression see its matches in a text.  There are also very helpful explainations to get you started. For the current example, we want to create a tokenization exception for a “<span class="math notranslate nohighlight">\(&quot; followed by numbers.  One way of expressing that is &quot;\$\d*&quot;, which will match any string with the charachter &quot;\)</span>” followed by digits (\d) repeating any number of times (*).  If you try it out in regex101, you’ll see that we get matches on “<span class="math notranslate nohighlight">\(1&quot;,&quot;\)</span>10000” and “$10000000”.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">TOKENIZER_PREFIXES</span> <span class="o">=</span> <span class="p">(</span>
    <span class="sa">r</span><span class="s2">&quot;\$\d*&quot;</span> 
<span class="p">)</span>
</pre></div>
</div>
<p>Wondering what the r is for?  It’s Python’s raw string, which treats a backslash as a literal character and won’t mistake it for a new line \n or tab \t or other escape charachters with a “” in them.</p>
<p>We can also add a rule for all of the currency symbols</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">spacy.lang.char_classes</span> <span class="kn">import</span> <span class="n">CURRENCY</span>

<span class="n">TOKENIZER_PREFIXES</span> <span class="o">=</span> <span class="p">(</span>
    <span class="sa">r</span><span class="s2">&quot;\</span><span class="si">{c}</span><span class="s2">\d*&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="n">CURRENCY</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>The code above will take each individual symbol in CURRENCY and add a regular expression rule for it by replacing {c} with the symbol.</p>
</div>
<div class="section" id="extending-defaults">
<h2><span class="section-number">3.6. </span>Extending defaults<a class="headerlink" href="#extending-defaults" title="Permalink to this headline">¶</a></h2>
<p>As you develop a language object for a new language, you can delegate much of the work to the existing defaults, but it’s quite likely that your language has its own specific symbols and charachters. You can add them to your language’s <code class="docutils literal notranslate"><span class="pre">punctuation.py</span></code> file.</p>
<p>For example, the default list of currency symbols does not include the Sheckel ₪. We can add our new prefix by adding a list of new symbols to our TOKENIZER_PREFIXES.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">spacy.lang.punctuation</span> <span class="kn">import</span> <span class="n">TOKENIZER_PREFIXES</span> <span class="k">as</span> <span class="n">BASE_TOKENIZER_PREFIXES</span>

<span class="n">TOKENIZER_PREFIXES</span> <span class="o">=</span> <span class="n">BASE_TOKENIZER_PREFIXES</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;₪&quot;</span><span class="p">]</span>
</pre></div>
</div>
<p>The prefixes are added to the language object in <code class="docutils literal notranslate"><span class="pre">__init__.py</span></code></p>
<div class="section" id="prefixes-in-action">
<h3><span class="section-number">3.6.1. </span>Prefixes in action<a class="headerlink" href="#prefixes-in-action" title="Permalink to this headline">¶</a></h3>
<p>The addition of prefixes will usually be added to the language object. However, you may want to make adjustments on the fly. It’s also a good way to check that the new prefix is really what you need <a class="reference external" href="https://spacy.io/usage/linguistic-features#native-tokenizer-additions">docs</a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">spacy</span> <span class="kn">import</span> <span class="n">util</span>
<span class="kn">from</span> <span class="nn">spacy.lang.he</span> <span class="kn">import</span> <span class="n">Hebrew</span>

<span class="n">nlp</span> <span class="o">=</span> <span class="n">Hebrew</span><span class="p">()</span>

<span class="n">prefixes</span> <span class="o">=</span> <span class="n">nlp</span><span class="o">.</span><span class="n">Defaults</span><span class="o">.</span><span class="n">prefixes</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;₪&#39;</span><span class="p">]</span>
<span class="n">prefix_regex</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">compile_prefix_regex</span><span class="p">(</span><span class="n">prefixes</span><span class="p">)</span>
<span class="n">nlp</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">prefix_search</span> <span class="o">=</span> <span class="n">prefix_regex</span><span class="o">.</span><span class="n">search</span>

<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;₪181 בלבד! משלוח חינם!&quot;</span><span class="p">)</span> <span class="c1">#&quot;Only NIS 181! Free Shipping!&quot;</span>
<span class="p">[</span><span class="n">t</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>[₪, 181, בלבד, !, משלוח, חינם, !]
</pre></div>
</div>
<p>Extra brain teaser: Hebrew is written from right to left, so why isn’t ₪ a suffix?
spaCy works well with RTL langauges, but the tokenizer moves from left to right.  Even though ₪ follows 181 in the sentence, spaCy considers it a prefix.</p>
</div>
<div class="section" id="infix">
<h3><span class="section-number">3.6.2. </span>Infix<a class="headerlink" href="#infix" title="Permalink to this headline">¶</a></h3>
<p>Off-pitch</p>
</div>
<div class="section" id="suffix">
<h3><span class="section-number">3.6.3. </span>Suffix<a class="headerlink" href="#suffix" title="Permalink to this headline">¶</a></h3>
</div>
</div>
</div>
<div class="section" id="building-a-new-language-tokenizer">
<h1><span class="section-number">4. </span>Building A New Language Tokenizer<a class="headerlink" href="#building-a-new-language-tokenizer" title="Permalink to this headline">¶</a></h1>
<p>In the previous section, we covered the key concepts that you need to create a tokenizer for your new language.  However, knowing what a brick is does not tell you how to build a house.  In this section, we’ll cover the process of building a new language object’s tokenizer. This process includes identifying term variations in your corpus</p>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./w1"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="intro-to-linguistic-data.html" title="previous page"><span class="section-number">2. </span>Introduction to Linguistic Data</a>
    <a class='right-next' id="next-link" href="annotation.html" title="next page"><span class="section-number">5. </span>Annotation with INCEpTION</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Natalia Ermolaev, Andrew Janco, Toma Tasovac, Quinn Dombrowski, David Lassner<br/>
        
          <div class="extra_footer">
            <p>
<a target="_blank" href="https://www.neh.gov/divisions/odh"><img src="https://www.neh.gov/sites/default/files/inline-files/NEH-Horizontal-Stk-Seal-Black820.jpg" /></a>
</p>

          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>