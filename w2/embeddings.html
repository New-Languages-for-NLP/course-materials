
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>25. Embeddings, Do You Need Them? &#8212; New Languages for NLP</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="27. Applied Session with Transformers" href="applied-embeddings.html" />
    <link rel="prev" title="24. 🌿 The New Language Project" href="using-inception-data/New%20Language%20Training.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">New Languages for NLP</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Welcome!
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Prerequisite Knowledge
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../prep/introduction.html">
   1. Getting Ready for the Workshops
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../prep/ml.html">
   2. Neural Networks and Machine Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../prep/nlp.html">
   3. NLP and spaCy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../prep/fairuse.html">
   4. Corpus Documentation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Prerequisite Skills
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../prep/data.html">
   5. Data Formats
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../prep/tei.html">
   6. Text Encoding Initiative (TEI)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../prep/github.html">
   7. GitHub
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../prep/jupyter.html">
   8. Python
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Workshop I Annotation and Linguistic Data ~ June 21-25, 2021.
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../w1/overview.html">
   9. Welcome to Workshop I
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../w1/cadet.html">
   10. Cadet
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../w1/cadet-notebook.html">
   11. Cadet the Notebook
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../w1/inception.html">
   12. INCEpTION
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../w1/tokenization.html">
   13. Tokenization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../w1/ud.html">
   14. Universal features
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../w1/InstituteGlossary.html">
   15. Institute Glossary
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Workshop II Model Training ~ January 10-14, 2022
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="overview.html">
   16. Welcome to Workshop II
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="prep.html">
   17. Preparation for Workshop II
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="colab.html">
   18. Using Colab
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="intro-ml.html">
   19. Introduction to Machine Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="practical-intro/Practical%20Introduction%20to%20Model%20Training.html">
   21. Practical Introduction to Model Training
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="projects.html">
   22. The Project File
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="config.html">
   23. The Config File
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="using-inception-data/New%20Language%20Training.html">
   24. 🌿 The New Language Project
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   25. Embeddings, Do You Need Them?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="applied-embeddings.html">
   27. Applied Session with Transformers
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Workshop III  Presentations and Publications ~ May 12-14, 2022
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../w3/overview.html">
   28. Welcome to Workshop III
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/w2/embeddings.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   25. Embeddings, Do You Need Them?
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#introduction">
     25.1. Introduction
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#transformers">
     25.2. Transformers
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-transformers-in-spacy">
   26. Using Transformers in spaCy
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#links">
     26.1. Links
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Embeddings, Do You Need Them?</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   25. Embeddings, Do You Need Them?
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#introduction">
     25.1. Introduction
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#transformers">
     25.2. Transformers
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-transformers-in-spacy">
   26. Using Transformers in spaCy
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#links">
     26.1. Links
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="embeddings-do-you-need-them">
<h1><span class="section-number">25. </span>Embeddings, Do You Need Them?<a class="headerlink" href="#embeddings-do-you-need-them" title="Permalink to this headline">#</a></h1>
<section id="introduction">
<h2><span class="section-number">25.1. </span>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">#</a></h2>
<p>lecture notes and slides will be added after the session.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>I find this visualization helps me to make sense of embeddings. It has English word2vec embeddings as well as the multilingual GNMT Interlingua embeddings.</p>
<p><a class="reference external" href="https://projector.tensorflow.org/">https://projector.tensorflow.org/</a></p>
</div>
</section>
<section id="transformers">
<h2><span class="section-number">25.2. </span>Transformers<a class="headerlink" href="#transformers" title="Permalink to this headline">#</a></h2>
<p>In June 2017, researchers from Google published a now famous paper titled <a class="reference external" href="https://arxiv.org/abs/1706.03762">“Attention is All You Need.”</a>.  In the text, they ouline a new kind of language model called a Transformer.  This new architecture builds on many existing methods, but as the title suggests, it gives pride of place to attention.  Simply put, attention is the ability to highlight or focus on particular elements over others in the input text. For example, let’s take the sentence: “My date left a novel in the park.” If we ask a machine to predict  each token’s part of speech, it needs to look at the words around each token to disambiguate the many possible meanings. In a sequence model, the model looks at the tokens one after another moving from the beginning to the end of the sentence.  Using attention, a model can highlight those elements of the preceeding sequence that aid in making predictions.  I know that “novel” is most likely a book (and not an adjective), because it is the subject of preceeding action.  But at the start of the sentence, we have very little to help us.  Is “My date” going to be “My date left” or “My date of birth?”  Transformers overcome this problem by being bi-directional. They can “see” the whole sentence when making predictions. They can also “remember” and attend to attributes across the corpus.  This is where transformers start to become very good at generating text as well as the “look and feel” of human languages. It still has no understanding of the content of the text, but it’s very good at understanding the patterns in the corpus and common traits.</p>
<p>I find this visualization helps to see what attention looks like in practice. This model is learning to translate from English to French. The model attends to corresponding words in the French and the English versions of the sentence. This allows the model to negotiate the different word order in <code class="docutils literal notranslate"><span class="pre">zone</span> <span class="pre">économique</span> <span class="pre">européenne</span></code> and <code class="docutils literal notranslate"><span class="pre">European</span> <span class="pre">Economic</span> <span class="pre">Area</span></code>.</p>
<img src="https://miro.medium.com/max/625/0*Zxi7-StjTuHDIg9I.png" />
<p>(<a class="reference external" href="https://arxiv.org/abs/1409.0473">source</a>)</p>
<p>The training of a transformer is very different from traditional sequence models.  Rather that providing training data with the correct answers, a transformer uses massive amounts of unlabelled text for training.  The model learns contextual embeddings by masking words (replacing a word with <code class="docutils literal notranslate"><span class="pre">[mask]</span></code>) and using the surrounding text to make a prediction. So our example becomes <code class="docutils literal notranslate"><span class="pre">My</span> <span class="pre">date</span> <span class="pre">left</span> <span class="pre">a</span> <span class="pre">[MASK]</span> <span class="pre">in</span> <span class="pre">the</span> <span class="pre">park.</span></code> The model can attend to earlier mentions of novels and books in the text to predict that the date left a novel rather than some other random noun. The Google researchers’ claim that “attention is all you need” highlights how powerful context can be. With enough data (all the Internet), we can train better models by attending to context rather than using supervised learning with labelled data.</p>
<p>Transformers are masters of autocomplete. Given a starting prompt, they can continue writing.  Not only is the text usually legible and makes sense, but it will include contextually relevant and sensible content. For live examples, I ecourage you to try <a class="reference external" href="https://transformer.huggingface.co/">write with transforer</a> and <a class="reference external" href="https://play.aidungeon.io/">AI dungeon</a>. On other tasks, these models of language have equally impressive capabilities even when their contributions are less visible.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/SZorAJ4I-sA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="using-transformers-in-spacy">
<h1><span class="section-number">26. </span>Using Transformers in spaCy<a class="headerlink" href="#using-transformers-in-spacy" title="Permalink to this headline">#</a></h1>
<p>In the <code class="docutils literal notranslate"><span class="pre">new_langproject/configs</span></code> folder in your GitHub repository, you’ll find a <code class="docutils literal notranslate"><span class="pre">transformer_config.cfg</span></code> file. To use this configuration, change the value in your project.yml file.  Change <code class="docutils literal notranslate"><span class="pre">config:</span> <span class="pre">&quot;config.cfg&quot;</span></code> to <code class="docutils literal notranslate"><span class="pre">config:</span> <span class="pre">&quot;transformer_config.cfg&quot;</span></code>. This is a file generated by the <code class="docutils literal notranslate"><span class="pre">init</span> <span class="pre">config</span></code> command with the <code class="docutils literal notranslate"><span class="pre">--gpu</span></code> tag.  You can also create a similar base config file using the <a class="reference external" href="https://spacy.io/usage/training#quickstart">Quickstart widget</a> and selecting GPU under hardware.</p>
<ul class="simple">
<li><p>One of the main changes you’ll see with this configuration is that the <code class="docutils literal notranslate"><span class="pre">tok2vec</span></code> component is replaced by <code class="docutils literal notranslate"><span class="pre">transformer</span></code>.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pipeline</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;tok2vec&quot;</span><span class="p">,</span><span class="s2">&quot;tagger&quot;</span><span class="p">,</span><span class="s2">&quot;parser&quot;</span><span class="p">,</span><span class="s2">&quot;ner&quot;</span><span class="p">]</span>
<span class="c1"># vs.</span>
<span class="n">pipeline</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">,</span><span class="s2">&quot;tagger&quot;</span><span class="p">,</span><span class="s2">&quot;parser&quot;</span><span class="p">,</span><span class="s2">&quot;ner&quot;</span><span class="p">]</span>
</pre></div>
</div>
<p>Further settings can be found in the <code class="docutils literal notranslate"><span class="pre">[components.transformer]</span></code> section:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">components</span><span class="o">.</span><span class="n">transformer</span><span class="p">]</span>
<span class="n">factory</span> <span class="o">=</span> <span class="s2">&quot;transformer&quot;</span>
<span class="n">max_batch_items</span> <span class="o">=</span> <span class="mi">4096</span>
<span class="n">set_extra_annotations</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;@annotation_setters&quot;</span><span class="p">:</span><span class="s2">&quot;spacy-transformers.null_annotation_setter.v1&quot;</span><span class="p">}</span>

<span class="p">[</span><span class="n">components</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">model</span><span class="p">]</span>
<span class="nd">@architectures</span> <span class="o">=</span> <span class="s2">&quot;spacy-transformers.TransformerModel.v3&quot;</span>
<span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;distilbert-base-multilingual-cased&quot;</span>
<span class="n">mixed_precision</span> <span class="o">=</span> <span class="n">false</span>
</pre></div>
</div>
<p>You do not need to change anything here to fetch a pre-trained multilingual transformer from Hugging Face. However, you can utilize many of the models on Hugging Face Hub.  Find the line below and change to the desired model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;distilbert-base-multilingual-cased&quot;</span>
</pre></div>
</div>
<p>For example, to change to a model trained on Harry Potter fan fiction, I’d visit Hugging Face <a class="reference external" href="https://huggingface.co/ceostroff/harry-potter-gpt2-fanfiction">here</a></p>
<img src="https://github.com/New-Languages-for-NLP/files/raw/main/fanfic.png" />
<p>If you click to the right of the model title you can copy the model name <code class="docutils literal notranslate"><span class="pre">ceostroff/harry-potter-gpt2-fanfiction</span></code>. Replace the <code class="docutils literal notranslate"><span class="pre">name</span></code> in <code class="docutils literal notranslate"><span class="pre">[components.transformer.model]</span></code> and spaCy will load this model.</p>
<p>Please note that not all models are compatible with spaCy’s architecture and this should be considered a highly advanced topic. Transformer models loaded into spaCy are only used for their contextual embeddings. The Harry Potter model will work well with similar texts and domains, but will not generate text.  If you find a model that’s been pre-trained for a specific task, spaCy will only inherit the underlying model, not the task-specific layers.  All of spaCy’s <a class="reference external" href="https://spacy.io/models">pre-trained transformers</a> should work out of the box.</p>
<p>If your activation function’s perpexity mixes Baysian logits against cross entropy layers, simply reshape your tensor with a nonlinearity using <a class="reference external" href="https://twitter.com/veekorbes/status/1410796865126346755?s=20">Infinidash</a>. Just joking, that’s all jargon salad.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>What if a transformer model does not exist for my language, domain or anything like it?</p>
<ol class="simple">
<li><p>You can try transfer learning by using an existing multi-lingual transformer or related-language transformer.</p></li>
<li><p>Train a transformer from scratch using the Oscar dataset with text in <a class="reference external" href="https://oscar-corpus.com/post/oscar-2019/">166 languages</a> from the web. Note that the amount of text for different languages will vary a lot.  Also you’re depending on automatic language identification.  Still, this is a not entirely crazy way to train your own transformer from scratch. You have been warned!
AJ update 14/1: The notebooks that I shared earlier trains a type of model that is not compatible with spaCy at the moment. I am going to create a notebook that does the same but trains a PyTorch Roberta model that should be compatible.  Stay tuned and let me know if this would be useful for your project.</p></li>
</ol>
<p>Also the notebook that I shared does not contain the code you’ll need to save the model.  You’ll need to add the following:</p>
<ol class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">!curl</span> <span class="pre">-s</span> <span class="pre">https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh</span> <span class="pre">|</span> <span class="pre">sudo</span> <span class="pre">bash</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">!apt-get</span> <span class="pre">install</span> <span class="pre">git-lfs</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">!huggingface-cli</span> <span class="pre">login</span></code> you’ll need to use a token from HF.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">!huggingface-cli</span> <span class="pre">repo</span> <span class="pre">create</span> <span class="pre">your-language-name</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model.save_pretrained('your-user/your-language-name',</span> <span class="pre">push_to_hub=True)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tokenizer.save_pretrained(&quot;your-user/your-language-name&quot;,</span> <span class="pre">push_to_hub=True)</span></code></p></li>
</ol>
</div>
<p>When training with transformer-based models, you should use GPUs or TPUs on Colab and be prepared for significantly larger training times (7 hours or more). However, for many tasks, the beneifts of pre-training and transfer learning should be very clear in the model metrics.</p>
<section id="links">
<h2><span class="section-number">26.1. </span>Links<a class="headerlink" href="#links" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://spacy.io/usage/embeddings-transformers">Embeddings, Transformers and Transfer Learning</a></p></li>
<li><p><a class="reference external" href="https://youtu.be/iDulhoQ2pro">Summary of “Attention is All You Need”</a></p></li>
<li><p><a class="reference external" href="https://www.wwp.northeastern.edu/outreach/seminars/neh_wem.html">Word Vectors for the Thoughtful Humanist</a></p></li>
<li><p><a class="reference external" href="https://melaniewalsh.github.io/BERT-for-Humanists/">BERT for Humanists</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./w2"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="using-inception-data/New%20Language%20Training.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">24. </span>🌿 The New Language Project</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="applied-embeddings.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">27. </span>Applied Session with Transformers</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Natalia Ermolaev, Andrew Janco, Toma Tasovac, Quinn Dombrowski, David Lassner<br/>
  
    <div class="extra_footer">
      <p>
<a target="_blank" href="https://www.neh.gov/divisions/odh"><img style="height:10%; width:10%" src="https://www.neh.gov/sites/default/files/inline-files/NEH-Horizontal-Stk-Seal-Black820.jpg" /></a>
<a target="_blank" href="https://cdh.princeton.edu"><img style="height:10%; width:10%" src="https://cdh.princeton.edu/static/img/CDH_logo.svg" /></a>
<a target="_blank" href="https://www.dariah.eu/"><img style="height:19%; width:19%" src="https://www.dariah.eu/wp-content/uploads/2017/02/logo.png" /></a>

</p>

    </div>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>