
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>25. Embeddings, Do You Need Them? &#8212; New Languages for NLP</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="27. Applied Session with Transformers" href="applied-embeddings.html" />
    <link rel="prev" title="24. 🌿 The New Language Project" href="using-inception-data/New%20Language%20Training.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">New Languages for NLP</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Welcome!
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Prerequisite Knowledge
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../prep/introduction.html">
   1. Getting Ready for the Workshops
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../prep/ml.html">
   2. Neural Networks and Machine Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../prep/nlp.html">
   3. NLP and spaCy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../prep/fairuse.html">
   4. Corpus Documentation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Prerequisite Skills
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../prep/data.html">
   5. Data Formats
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../prep/tei.html">
   6. Text Encoding Initiative (TEI)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../prep/github.html">
   7. GitHub
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../prep/jupyter.html">
   8. Python
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Workshop I Annotation and Linguistic Data ~ June 21-25, 2021.
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../w1/overview.html">
   9. Welcome to Workshop I
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../w1/cadet.html">
   10. Cadet
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../w1/cadet-notebook.html">
   11. Cadet the Notebook
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../w1/inception.html">
   12. INCEpTION
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../w1/tokenization.html">
   13. Tokenization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../w1/ud.html">
   14. Universal features
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../w1/InstituteGlossary.html">
   15. Institute Glossary
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Workshop II Model Training ~ January 10-14, 2022
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="overview.html">
   16. Welcome to Workshop II
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="prep.html">
   17. Preparation for Workshop II
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="colab.html">
   18. Using Colab
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="intro-ml.html">
   19. Introduction to Machine Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="practical-intro/Practical%20Introduction%20to%20Model%20Training.html">
   21. Practical Introduction to Model Training
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="projects.html">
   22. The Project File
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="config.html">
   23. The Config File
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="using-inception-data/New%20Language%20Training.html">
   24. 🌿 The New Language Project
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   25. Embeddings, Do You Need Them?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="applied-embeddings.html">
   27. Applied Session with Transformers
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Workshop III  Presentations and Publications ~ May 12-14, 2022
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../w3/overview.html">
   28. Welcome to Workshop III
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/w2/embeddings.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   25. Embeddings, Do You Need Them?
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#introduction">
     25.1. Introduction
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tok2vec">
     25.2. Tok2Vec
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#transformers">
     25.3. Transformers
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-transformers-in-spacy">
   26. Using Transformers in spaCy
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#links">
     26.1. Links
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Embeddings, Do You Need Them?</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   25. Embeddings, Do You Need Them?
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#introduction">
     25.1. Introduction
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tok2vec">
     25.2. Tok2Vec
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#transformers">
     25.3. Transformers
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-transformers-in-spacy">
   26. Using Transformers in spaCy
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#links">
     26.1. Links
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="embeddings-do-you-need-them">
<h1><span class="section-number">25. </span>Embeddings, Do You Need Them?<a class="headerlink" href="#embeddings-do-you-need-them" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction">
<h2><span class="section-number">25.1. </span>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>more soon…</p>
</div>
<div class="section" id="tok2vec">
<h2><span class="section-number">25.2. </span>Tok2Vec<a class="headerlink" href="#tok2vec" title="Permalink to this headline">¶</a></h2>
<p>The standard spaCy model uses a trainable component called Tok2Vec.  These are numerical representations of spaCy Token objects.  Similar to word embeddings,these tensors can be adjusted to store semantic information such as similarity, meaning or sentiment. During training, these tensors can be updated given information from other pipeline components to facilitate better performance.</p>
</div>
<div class="section" id="transformers">
<h2><span class="section-number">25.3. </span>Transformers<a class="headerlink" href="#transformers" title="Permalink to this headline">¶</a></h2>
<p>In June 2017, researchers from Google published a now famous paper titled <a class="reference external" href="https://arxiv.org/abs/1706.03762">“Attention is All You Need.”</a>.  In the text, they ouline a new kind of language model called a Transformer.  This new architecture builds on many existing methods, but as the title suggests, it gives pride of place to attention.  Simply put, attention is the ability to highlight or focus on particular elements over others in the input text. For example, let’s take the sentence: “My date left a novel in the park.” If we ask a machine to predict  each token’s part of speech, it needs to look at the words around each token to disambiguate the many possible meanings. In a sequence model, the model looks at the tokens one after another moving from the beginning to the end of the sentence.  Using attention, a model can highlight those elements of the preceeding sequence that aid in making predictions.  I know that “novel” is most likely a book, because it is the subject of preceeding action.  But at the start of the sentence, we have very little to help us.  Is “My date” going to be “My date left” or “My date of birth?”  Transformers overcome this problem by being bi-directional. They can “see” the whole sentence when making predictions.</p>
<p>The training of a transformer is very different from traditional sequence models.  Rather that providing training data with the correct answers, a transformer uses massive amounts of unlabelled text for training.  The model learns contextual embeddings by masking words and using the surrounding text to make a prediction. So our example becomes <code class="docutils literal notranslate"><span class="pre">My</span> <span class="pre">date</span> <span class="pre">left</span> <span class="pre">a</span> <span class="pre">[MASK]</span> <span class="pre">in</span> <span class="pre">the</span> <span class="pre">park.</span></code> The model can attend to earlier mentions of novels and books in the text to predict that the date left a novel rather than some other random noun.  The Google researchers’ claim that “attention is all you need” highlights how powerful context can be. With enough data (all the Internet), we can train better models by attending to context rather than using supervised learning with labelled data.</p>
<p>Transformers are masters of autocomplete. Given a starting prompt, they can continue writing.  Not only is the text usually legible and makes sense, but it will include contextually relevant and sensible content. For live examples, I ecourage you to try <a class="reference external" href="https://transformer.huggingface.co/">write with transforer</a> and <a class="reference external" href="https://play.aidungeon.io/">AI dungeon</a>. On other tasks, these models of language have equally impressive capabilities even when their contributions are less visible.</p>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="using-transformers-in-spacy">
<h1><span class="section-number">26. </span>Using Transformers in spaCy<a class="headerlink" href="#using-transformers-in-spacy" title="Permalink to this headline">¶</a></h1>
<p>In the <code class="docutils literal notranslate"><span class="pre">new_langproject/configs</span></code> folder in your GitHub repository, you’ll find a <code class="docutils literal notranslate"><span class="pre">transformer_config.cfg</span></code> file. To use this configuration, change the value in your project.yml file.  Change <code class="docutils literal notranslate"><span class="pre">config:</span> <span class="pre">&quot;config.cfg&quot;</span></code> to <code class="docutils literal notranslate"><span class="pre">config:</span> <span class="pre">&quot;transformer_config.cfg&quot;</span></code>. This is a file generated by the <code class="docutils literal notranslate"><span class="pre">init</span> <span class="pre">config</span></code> command with the <code class="docutils literal notranslate"><span class="pre">--gpu</span></code> tag.  You can also create a similar base config file using the <a class="reference external" href="https://spacy.io/usage/training#quickstart">Quickstart widget</a> and selecting GPU under hardware.</p>
<ul class="simple">
<li><p>One of the main changes you’ll see with this configuration is that the <code class="docutils literal notranslate"><span class="pre">tok2vec</span></code> component is replaced by <code class="docutils literal notranslate"><span class="pre">transformer</span></code>.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pipeline</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;tok2vec&quot;</span><span class="p">,</span><span class="s2">&quot;tagger&quot;</span><span class="p">,</span><span class="s2">&quot;parser&quot;</span><span class="p">,</span><span class="s2">&quot;ner&quot;</span><span class="p">]</span>
<span class="c1"># vs.</span>
<span class="n">pipeline</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">,</span><span class="s2">&quot;tagger&quot;</span><span class="p">,</span><span class="s2">&quot;parser&quot;</span><span class="p">,</span><span class="s2">&quot;ner&quot;</span><span class="p">]</span>
</pre></div>
</div>
<p>Further settings can be found in the <code class="docutils literal notranslate"><span class="pre">[components.transformer]</span></code> section:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">components</span><span class="o">.</span><span class="n">transformer</span><span class="p">]</span>
<span class="n">factory</span> <span class="o">=</span> <span class="s2">&quot;transformer&quot;</span>
<span class="n">max_batch_items</span> <span class="o">=</span> <span class="mi">4096</span>
<span class="n">set_extra_annotations</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;@annotation_setters&quot;</span><span class="p">:</span><span class="s2">&quot;spacy-transformers.null_annotation_setter.v1&quot;</span><span class="p">}</span>

<span class="p">[</span><span class="n">components</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">model</span><span class="p">]</span>
<span class="nd">@architectures</span> <span class="o">=</span> <span class="s2">&quot;spacy-transformers.TransformerModel.v3&quot;</span>
<span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;distilbert-base-multilingual-cased&quot;</span>
<span class="n">mixed_precision</span> <span class="o">=</span> <span class="n">false</span>
</pre></div>
</div>
<p>You do not need to change anything here to fetch a pre-trained multilingual transformer from Hugging Face. However, you can utilize many of the models on Hugging Face Hub.  Find the line below and change to the desired model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;distilbert-base-multilingual-cased&quot;</span>
</pre></div>
</div>
<p>For example, to change to a model trained on Harry Potter fan fiction, I’d visit Hugging Face <a class="reference external" href="https://huggingface.co/ceostroff/harry-potter-gpt2-fanfiction">here</a></p>
<img src="https://github.com/New-Languages-for-NLP/files/raw/main/fanfic.png" />
<p>If you click to the right of the model title you can copy the model name <code class="docutils literal notranslate"><span class="pre">ceostroff/harry-potter-gpt2-fanfiction</span></code>. Replace the <code class="docutils literal notranslate"><span class="pre">name</span></code> in <code class="docutils literal notranslate"><span class="pre">[components.transformer.model]</span></code> and spaCy will load this model.</p>
<p>Please note that not all models are compatible with spaCy’s architecture and this should be considered a highly advanced topic. Transformer models loaded into spaCy are only used for their contextual embeddings. The Harry Potter model will work well with similar texts and domains, but will not generate text.  If you find a model that’s been pre-trained for a specific task, spaCy will only inherit the underlying model, not the task-specific layers.  All of spaCy’s <a class="reference external" href="https://spacy.io/models">pre-trained transformers</a> should work out of the box.</p>
<p>If your activation function’s perpexity mixes Baysian logits against cross entropy layers, simply reshape your tensor with a nonlinearity using <a class="reference external" href="https://twitter.com/veekorbes/status/1410796865126346755?s=20">Infinidash</a>. Just joking, that’s all jargon salad.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>What if a transformer model does not exist for my language, domain or anything like it?</p>
<ol class="simple">
<li><p>You can try transfer learning by using an existing multi-lingual transformer.</p></li>
<li><p>Train a transformer using <a class="reference external" href="https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/causal_language_modeling_flax.ipynb">this notebook</a> using data the Oscar dataset with text in 166 languages from the web. Note that the amount for text for different languages will vary a lot.  Also you’re depending on automatic language identification.  Still, this is a not entirely crazy way to train your own transformer from scratch. You have been warned!</p></li>
</ol>
</div>
<p>When training with transformer-based models, you should use GPUs on Colab and be prepared for significantly larger training times. However, for many tasks, the beneifts of pre-training and transfer learning should be very clear in the model metrics.</p>
<div class="section" id="links">
<h2><span class="section-number">26.1. </span>Links<a class="headerlink" href="#links" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://spacy.io/usage/embeddings-transformers">Embeddings, Transformers and Transfer Learning</a></p></li>
<li><p><a class="reference external" href="https://youtu.be/iDulhoQ2pro">Summary of “Attention is All You Need”</a></p></li>
<li><p><a class="reference external" href="https://www.wwp.northeastern.edu/outreach/seminars/neh_wem.html">Word Vectors for the Thoughtful Humanist</a></p></li>
<li><p><a class="reference external" href="https://melaniewalsh.github.io/BERT-for-Humanists/">BERT for Humanists</a></p></li>
<li><p><a class="reference external" href="https://explosion.ai/blog/spacy-transformers">spaCy meets Transformers: Fine-tune BERT, XLNet and GPT-2</a></p></li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./w2"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="using-inception-data/New%20Language%20Training.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">24. </span>🌿 The New Language Project</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="applied-embeddings.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">27. </span>Applied Session with Transformers</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Natalia Ermolaev, Andrew Janco, Toma Tasovac, Quinn Dombrowski, David Lassner<br/>
    
      <div class="extra_footer">
        <p>
<a target="_blank" href="https://www.neh.gov/divisions/odh"><img style="height:10%; width:10%" src="https://www.neh.gov/sites/default/files/inline-files/NEH-Horizontal-Stk-Seal-Black820.jpg" /></a>
<a target="_blank" href="https://cdh.princeton.edu"><img style="height:10%; width:10%" src="https://cdh.princeton.edu/static/img/CDH_logo.svg" /></a>
<a target="_blank" href="https://www.dariah.eu/"><img style="height:19%; width:19%" src="https://www.dariah.eu/wp-content/uploads/2017/02/logo.png" /></a>

</p>

      </div>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>