{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "061ba15a",
   "metadata": {},
   "source": [
    "ðŸŒ¿ The New Language Project\n",
    "=======================\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1iC2YF3s30e0lDxdGmegRKS8iggLldMMH?usp=sharing)\n",
    "\n",
    "For our workshops, we've created a spaCy project file for you that will:\n",
    "- fetch your language data from GitHub\n",
    "- convert and prepare the data for model training\n",
    "- train a model for your new language\n",
    "- package and publish your new model\n",
    "\n",
    "This workflow can be adapted to meet the specific needs of your project. In this section, we will walk through the various sections and scripts of the project.  We've made some choices on your behalf. They may be right, or you may want to change things. Let's see what's there.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca9a0bd",
   "metadata": {},
   "source": [
    "In your language team's GitHub repository, you'll find a `newlang_project` folder that contains a `project.yml` file. In the next cell, we're going to clone the repository and fetch the project's assets. If your repository is private, you'll need to get a developer key and enter it as `git_access_token`.  Otherwise, just enter your repository's name under `repo_name`. \n",
    "```\n",
    "newlang_project\n",
    "â”‚   README.md\n",
    "â”‚   project.yml    \n",
    "â”‚\n",
    "â””â”€â”€â”€scripts\n",
    "    â”‚   convert.py\n",
    "    â”‚   split.py\n",
    "    â”‚   update_config.py\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c0bda8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2mâœ” Cloned 'newlang_project' from New-Languages-for-NLP/repo-template\u001b[0m\n",
      "/srv/projects/course-materials/w2/using-inception-data/newlang_project\n",
      "\u001b[38;5;2mâœ” Your project is now ready!\u001b[0m\n",
      "To fetch the assets, run:\n",
      "python -m spacy project assets /srv/projects/course-materials/w2/using-inception-data/newlang_project\n",
      "\u001b[38;5;4mâ„¹ Fetching 1 asset(s)\u001b[0m\n",
      "\u001b[38;5;2mâœ” Downloaded asset\n",
      "/srv/projects/course-materials/w2/using-inception-data/newlang_project/assets/urban-giggle\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "repo_name = \"repo-template\"\n",
    "private_repo = False\n",
    "git_access_token = \"\"\n",
    "\n",
    "!rm -rf /content/newlang_project\n",
    "!rm -rf $repo_name\n",
    "if private_repo:\n",
    "    git_url = f\"https://{git_access_token}@github.com/New-Languages-for-NLP/{repo_name}/\"\n",
    "    !git clone $git_url  -b main\n",
    "    !cp -r ./$repo_name/newlang_project .  \n",
    "    !mkdir newlang_project/assets/\n",
    "    !mkdir newlang_project/configs/\n",
    "    !mkdir newlang_project/corpus/\n",
    "    !mkdir newlang_project/metrics/\n",
    "    !mkdir newlang_project/packages/\n",
    "    !mkdir newlang_project/training/\n",
    "    !mkdir newlang_project/assets/$repo_name\n",
    "    !cp -r ./$repo_name/* newlang_project/assets/$repo_name/\n",
    "    !rm -rf ./$repo_name\n",
    "else:\n",
    "    !python -m spacy project clone newlang_project --repo https://github.com/New-Languages-for-NLP/$repo_name --branch main\n",
    "    !python -m spacy project assets /srv/projects/course-materials/w2/using-inception-data/newlang_project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa5ccc7",
   "metadata": {},
   "source": [
    "Let's start with the **project.yml** file in the newlang_project folder. \n",
    "\n",
    "You'll find a **metadata** section that you can update however you like using the yaml format.  \n",
    "\n",
    "```yaml\n",
    "title: \"Train new language model from cadet and inception data\"\n",
    "description: \"This project template lets you train a part-of-speech tagger, morphologizer and dependency parser from your cadet and inception data.\"\n",
    "```\n",
    "\n",
    "The **vars** section will have some information that is specific to your team. \n",
    "\n",
    "```yaml \n",
    "vars:\n",
    "  config: \"config\"\n",
    "  lang: \"yi\"\n",
    "  treebank: \"yiddish\"\n",
    "  test_size: 0.2\n",
    "  n_sents: 10\n",
    "  random_state: 11\n",
    "  package_name: \"Yiddish NewNLP Model May 2022\"\n",
    "  package_version: \"0.1\"\n",
    "  wandb: true \n",
    "  gpu: -1\n",
    "```\n",
    "\n",
    "- The `config` setting is the name and location of the config file.  We'll just have `config.cfg` in the project directory, so nothing fancy here. \n",
    "- `lang` is the ISO-style abbreviation for your language. \n",
    "- `treebank` is the name of your language's repository (and is ususally the same as the language name).\n",
    "- `test_size` is the percentage of data that you want to set aside for model validation and testing. An 80/20 split is a good place to start, so you'll see it set initially to `0.2`. For more, this [stackoverflow discussion](https://stackoverflow.com/questions/13610074/is-there-a-rule-of-thumb-for-how-to-divide-a-dataset-into-training-and-validatio) is very informative. \n",
    "- To evenly distribute your texts between the training and validation datasets, we split each text into blocks of 10 sentences. This is defined by the `n_sents` variable.\n",
    "- To ensure that the test and train split is consistent and reproducible, we use a number called `random_state`. More [here](https://scikit-learn.org/stable/glossary.html#term-random_state).\n",
    "- The `package_name` is used during packaging. It sets the package's metadata name. Basically, what is the name of your language model? \n",
    "- Similarly, `package_version` sets the package metadata for version.\n",
    "- spaCy comes with some basic ways to log training data.  However, [Weights and Biases](https://wandb.ai/) provides an excellent way to record, manage and share experiment data. You'll need to `pip install wanb`, create a free account and get an API key to use bandb. Then change the `[training.logger]` section in your config file from \n",
    "\n",
    "```yaml\n",
    "@loggers = \"spacy.ConsoleLogger.v1\"\n",
    "\n",
    "# to \n",
    "\n",
    "@loggers = \"spacy.WandbLogger.v2\"\n",
    "project_name = \"{treebank}\"\n",
    "remove_config_values = []\n",
    "log_dataset_dir = \"./assets\"\n",
    "model_log_interval = 1000 # optional to save checkpoint files to wanb\n",
    "```\n",
    "- Finally, model training with graphics chips (GPUs) is often faster than with a standard CPU. We recommend using Colab for their free GPUs.  In such a case, you'd change `-1` (CPU) to `0` (the GPU id).        \n",
    "\n",
    "\n",
    "**Assets** is configured to use your language repo name to fetch project data from GitHub.  It will save all that data in the `assets/your-language-name` folder.   \n",
    "\n",
    "The **commands** section is the heart of the project file.  Let's take some time to understand each command and what it does. \n",
    "\n",
    "---\n",
    "\n",
    "### Install \n",
    "\n",
    "The `install` command in the next cell will read the files in the `2_new_language_object` directory and will install the customized spaCy language object that you created for your language in Cadet. The language object will tell spaCy how to break your texts into tokens and sentence spans.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dc13741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "================================== install ==================================\u001b[0m\n",
      "Running command: rm -rf lang\n",
      "Running command: mkdir lang\n",
      "Running command: mkdir lang/yi\n",
      "Running command: cp -r assets/urban-giggle/2_new_language_object/ lang/yi/yi\n",
      "Running command: mv lang/yi/yi/setup.py lang/yi/\n",
      "Running command: /srv/projects/course-materials/w2/venv/bin/python -m pip install -e lang/yi\n",
      "Obtaining file:///srv/projects/course-materials/w2/using-inception-data/newlang_project/lang/yi\n",
      "Installing collected packages: yi\n",
      "  Attempting uninstall: yi\n",
      "    Found existing installation: yi 0.0.0\n",
      "    Uninstalling yi-0.0.0:\n",
      "      Successfully uninstalled yi-0.0.0\n",
      "  Running setup.py develop for yi\n",
      "Successfully installed yi\n"
     ]
    }
   ],
   "source": [
    "# Install the custom language object from Cadet \n",
    "!python -m spacy project run install /srv/projects/course-materials/w2/using-inception-data/newlang_project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada86268",
   "metadata": {},
   "source": [
    "### Config \n",
    "\n",
    "The `config` command creates a generic `config.cfg` file (for more on config files, see the [Config section](https://new-languages-for-nlp.github.io/course-materials/w2/config.html) in these course materials).  It updates the `train` and `dev` settings in the config file to point to the `train.spacy` and `dev.spacy` files that are created by the `split` command.  If you're using Weights and Biases, it will also change the `training logger`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be6828d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "=================================== config ===================================\u001b[0m\n",
      "Running command: /srv/projects/course-materials/w2/venv/bin/python -m spacy init config config.cfg --lang yi -F\n",
      "\u001b[38;5;3mâš  To generate a more effective transformer-based config (GPU-only),\n",
      "install the spacy-transformers package and re-run this command. The config\n",
      "generated now does not use transformers.\u001b[0m\n",
      "\u001b[38;5;4mâ„¹ Generated config template specific for your use case\u001b[0m\n",
      "- Language: yi\n",
      "- Pipeline: tagger, parser, ner\n",
      "- Optimize for: efficiency\n",
      "- Hardware: CPU\n",
      "- Transformer: None\n",
      "\u001b[38;5;2mâœ” Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2mâœ” Saved config\u001b[0m\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n",
      "Running command: /srv/projects/course-materials/w2/venv/bin/python scripts/update_config.py urban-giggle false\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy project run config /srv/projects/course-materials/w2/using-inception-data/newlang_project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ff594b",
   "metadata": {},
   "source": [
    "### Convert\n",
    "\n",
    "`Convert` will fetch your CoNLL-U and CoNLL 2002 (ner) files from the `3_inception_export` folder.  It creates a spaCy Doc object for each text and then splits the Doc into separate documents with 10 sentences each. For each text file, the `convert` script will look for a CoNLL 2002 file with the same name.  If that text exists, it will add the named entity data in the file to the existing Doc objects. It will then save all the Docs to disk using the `.spacy` binary format. \n",
    "The outcome is a `.spacy` for each text that includes the tokenization, sents, part of speech, lemma, morphology and named entity data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "563fdc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "================================== convert ==================================\u001b[0m\n",
      "Running command: /srv/projects/course-materials/w2/venv/bin/python scripts/convert.py assets/urban-giggle/3_inception_export 10 yi\n",
      "\u001b[38;5;4mâ„¹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2mâœ” Generated output file (49 documents):\n",
      "corpus/converted/he_htb-ud-dev.spacy\u001b[0m\n",
      "\u001b[38;5;4mâ„¹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2mâœ” Generated output file (50 documents):\n",
      "corpus/converted/he_htb-ud-test.spacy\u001b[0m\n",
      "\u001b[38;5;4mâ„¹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2mâœ” Generated output file (525 documents):\n",
      "corpus/converted/he_htb-ud-train.spacy\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy project run convert /srv/projects/course-materials/w2/using-inception-data/newlang_project -F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f9ea1d",
   "metadata": {},
   "source": [
    "### Split \n",
    "\n",
    "The `split` command loads all of the `.spacy` files and creates a list of Doc objects.  We then randomly shuffle them so that different kinds of text are evenly distibuted across the corpus.  Using a [`test_train_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function, we divide the corpus into a training and validation set. The split is determined by the `test_size` variable. The model will learn how to make accurate predictions using the training data. We then use the validation set to assess how well the model performs on completeley new and unseen data. We want the model to learn general rules and patterns rather than overfitting on one particular set of data. The validation set provides a measure of model improvement as part of the training process. Because the model has seen this data before, it's no longer useful as a tool to evaluate the trained model's performance.  So before we get started training, we also set aside 20% of the validation data to make a test set.  This final set of totally unseen data lets us measure how well the model has learned what we've asked it to learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9519c858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "=================================== split ===================================\u001b[0m\n",
      "Running command: /srv/projects/course-materials/w2/venv/bin/python scripts/split.py 0.2 11 yi\n",
      "ðŸš‚ Created 499 training docs\n",
      "ðŸ˜Š Created 100 validation docs\n",
      "ðŸ§ª  Created 25 test docs\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy project run split /srv/projects/course-materials/w2/using-inception-data/newlang_project -F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013c1fb7",
   "metadata": {},
   "source": [
    "### Debug \n",
    "\n",
    "The `debug` command runs `spacy debug data`, which provides a good overview of your prepared data.  This can help identify problems that will lead to poor model training. It's a good check-in and moment of reflection on the state of your data before moving forward. For more, see the [spaCy docs](https://spacy.io/api/cli#debug-data). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4feefe6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "=================================== debug ===================================\u001b[0m\n",
      "Running command: /srv/projects/course-materials/w2/venv/bin/python -m spacy debug data ./config.cfg\n",
      "\u001b[1m\n",
      "============================ Data file validation ============================\u001b[0m\n",
      "\u001b[38;5;2mâœ” Pipeline can be initialized with data\u001b[0m\n",
      "\u001b[38;5;2mâœ” Corpus is loadable\u001b[0m\n",
      "\u001b[1m\n",
      "=============================== Training stats ===============================\u001b[0m\n",
      "Language: yi\n",
      "Training pipeline: tok2vec, tagger, parser, ner\n",
      "499 training docs\n",
      "100 evaluation docs\n",
      "\u001b[38;5;2mâœ” No overlap between training and evaluation data\u001b[0m\n",
      "\u001b[38;5;3mâš  Low number of examples to train a new pipeline (499)\u001b[0m\n",
      "\u001b[1m\n",
      "============================== Vocab & Vectors ==============================\u001b[0m\n",
      "\u001b[38;5;4mâ„¹ 130313 total word(s) in the data (15962 unique)\u001b[0m\n",
      "\u001b[38;5;3mâš  30077 misaligned tokens in the training data\u001b[0m\n",
      "\u001b[38;5;3mâš  5675 misaligned tokens in the dev data\u001b[0m\n",
      "\u001b[38;5;4mâ„¹ No word vectors present in the package\u001b[0m\n",
      "\u001b[1m\n",
      "========================== Named Entity Recognition ==========================\u001b[0m\n",
      "\u001b[38;5;4mâ„¹ 0 label(s)\u001b[0m\n",
      "0 missing value(s) (tokens with '-' label)\n",
      "\u001b[38;5;2mâœ” Good amount of examples for all labels\u001b[0m\n",
      "\u001b[38;5;2mâœ” Examples without occurrences available for all labels\u001b[0m\n",
      "\u001b[38;5;2mâœ” No entities consisting of or starting/ending with whitespace\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Part-of-speech Tagging ===========================\u001b[0m\n",
      "\u001b[38;5;4mâ„¹ 15 label(s) in train data\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Dependency Parsing =============================\u001b[0m\n",
      "\u001b[38;5;4mâ„¹ Found 4714 sentence(s) with an average length of 27.6 words.\u001b[0m\n",
      "\u001b[38;5;4mâ„¹ Found 112 nonprojective train sentence(s)\u001b[0m\n",
      "\u001b[38;5;4mâ„¹ 36 label(s) in train data\u001b[0m\n",
      "\u001b[38;5;4mâ„¹ 52 label(s) in projectivized train data\u001b[0m\n",
      "\u001b[38;5;3mâš  Low number of examples for label 'dislocated' (10)\u001b[0m\n",
      "\u001b[38;5;3mâš  Low number of examples for label 'csubj' (1)\u001b[0m\n",
      "\u001b[38;5;3mâš  Low number of examples for label 'discourse' (2)\u001b[0m\n",
      "\u001b[38;5;3mâš  Low number of examples for 13 label(s) in the projectivized\n",
      "dependency trees used for training. You may want to projectivize labels such as\n",
      "punct before training in order to improve parser performance.\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Summary ==================================\u001b[0m\n",
      "\u001b[38;5;2mâœ” 6 checks passed\u001b[0m\n",
      "\u001b[38;5;3mâš  10 warnings\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy project run debug  /srv/projects/course-materials/w2/using-inception-data/newlang_project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c15220",
   "metadata": {},
   "source": [
    "### Train \n",
    "\n",
    "The `train` command is the moment we've all been waiting for. Go ahead and press the launch button! ðŸš€ This step will train the model using the settings in the config file.  \n",
    "\n",
    "When training begins, you'll see a bunch of numbers. Let's make sense of what they're saying.\n",
    "\n",
    "You'll see a list of what components are currently being trained.  `Pipeline: ['tok2vec', 'tagger', 'parser', 'ner']` Tok2vec are token embeddings or numerical representations of tokens that can be used efficiently by the model. The tagger will learn to predict part of speech values for your tokens. The parser will learn to predict grammatical structure. The ner component learns to predict named entities in the text. \n",
    "\n",
    "For each of these components, spaCy will print training metrics. So let's dive into this pile of forbidding verbiage and numbers.\n",
    "\n",
    "```\n",
    "E    #       LOSS TOK2VEC  LOSS TAGGER  LOSS PARSER  LOSS NER  TAG_ACC  DEP_UAS  DEP_LAS  SENTS_F  ENTS_F  ENTS_P  ENTS_R  SCORE\n",
    "```\n",
    "- The `E` refers to the epoch. An epoch is one complete pass of all the data through the model. You can set the number of epochs to complete during training or let spaCy optimize the number of epochs automatically (this is the default). \n",
    "- Every 200 examples, spaCy outputs accuracy scores in the `#` column. \n",
    "- `LOSS` refers to training loss. Loss is a measure of error. During training, the model will try to learn how to improve its predictions. Decreasing loss can suggest that the model is learning and improving.  If the loss value flattens or plateaus, the model has probably stopped learning or reached the best result for a given set of parameters and data.  You will find a loss measure for each of the pipeline components being trained. If the loss varies greatly and looks like a zigzag, the model is struggling to improve its predictions in a deliberate manner. `LOSS TOK2VEC  LOSS TAGGER  LOSS PARSER  LOSS NER`\n",
    "- `TAG_ACC` refers to the accuracy of the tagger component. [Accuracy](https://developers.google.com/machine-learning/glossary#accuracy) is the number of correct predictions divided by the total number of predictions made.\n",
    "- `DEP_UAS` and  `DEP_LAS` are the unlabeled attachment score (UAS) and labeled attachment score (LAS) for the dependency parser. This is a measure of how many times the model correctly predicted the correct head.\n",
    "- `SENTS_F` gives the model's [f-score](https://en.wikipedia.org/wiki/F-score) for sentence prediction.     \n",
    "- `ENTS_F  ENTS_P  ENTS_R` relate to the model's predictions of named entities. The f-score is the mean of precision and recall.   \n",
    "- Finally, spaCy logs a `SCORE` for the model's predictions overall. This gives a rough number for the model's overall accuracy.  As a general rule, increasing numbers means that the model is improving. By default, spaCy will end training when the score stops rising. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d32abf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "=================================== train ===================================\u001b[0m\n",
      "Running command: /srv/projects/course-materials/w2/venv/bin/python -m spacy train config.cfg --output training/urban-giggle --gpu-id -1 --nlp.lang=yi\n",
      "\u001b[38;5;4mâ„¹ Saving to output directory: training/urban-giggle\u001b[0m\n",
      "\u001b[38;5;4mâ„¹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2021-12-30 21:32:00,090] [INFO] Set up nlp object from config\n",
      "[2021-12-30 21:32:00,097] [INFO] Pipeline: ['tok2vec', 'tagger', 'parser']\n",
      "[2021-12-30 21:32:00,100] [INFO] Created vocabulary\n",
      "[2021-12-30 21:32:00,101] [INFO] Finished initializing nlp object\n",
      "[2021-12-30 21:32:04,936] [INFO] Initialized pipeline components: ['tok2vec', 'tagger', 'parser']\n",
      "\u001b[38;5;2mâœ” Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mâ„¹ Pipeline: ['tok2vec', 'tagger', 'parser']\u001b[0m\n",
      "\u001b[38;5;4mâ„¹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS TAGGER  LOSS PARSER  TAG_ACC  DEP_UAS  DEP_LAS  SENTS_F  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  -----------  -----------  -------  -------  -------  -------  ------  ------  ------  ------\n",
      "  0       0          0.00       145.60       431.08    22.31     3.87     3.16     0.09    0.00    0.00    0.00    0.09\n",
      "  0     200       2489.76     11513.39     26848.70    51.98    22.62    15.10    60.33    0.00    0.00    0.00    0.24\n",
      "  0     400       4685.01      6464.16     22632.98    56.50    25.11    19.89    77.27    0.00    0.00    0.00    0.26\n",
      "\u001b[38;5;2mâœ” Saved pipeline to output directory\u001b[0m\n",
      "training/urban-giggle/model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy project run train /srv/projects/course-materials/w2/using-inception-data/newlang_project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b00edf9",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "The **evaluate** command takes the trained model and tests it with the test data.  Recall that these are examples that the model has never seen, so they provide the best measure of its performance. The output will be saved as a json file in the metrics folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "018362d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "================================== evaluate ==================================\u001b[0m\n",
      "Running command: /srv/projects/course-materials/w2/venv/bin/python -m spacy evaluate ./training/urban-giggle/model-best ./corpus/converted/test.spacy --output ./metrics/urban-giggle.json --gpu-id -1\n",
      "\u001b[38;5;4mâ„¹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Results ==================================\u001b[0m\n",
      "\n",
      "TOK      80.58\n",
      "TAG      55.30\n",
      "UAS      23.93\n",
      "LAS      18.56\n",
      "SENT P   74.31\n",
      "SENT R   85.60\n",
      "SENT F   79.55\n",
      "SPEED    19651\n",
      "\n",
      "\u001b[1m\n",
      "=============================== LAS (per type) ===============================\u001b[0m\n",
      "\n",
      "                      P       R       F\n",
      "det               72.41    3.12    5.97\n",
      "nsubj             48.22   27.78   35.25\n",
      "flat:name         26.79   32.97   29.56\n",
      "root              61.47   56.80   59.04\n",
      "case:acc          88.46   28.40   42.99\n",
      "obj               39.29   13.75   20.37\n",
      "case:gen          86.79   27.88   42.20\n",
      "nmod:poss         42.31    8.09   13.58\n",
      "case              89.58   11.78   20.82\n",
      "obl               43.40    5.24    9.35\n",
      "nmod              28.00    2.32    4.28\n",
      "amod              43.48   20.41   27.78\n",
      "mark              70.27   14.29   23.74\n",
      "acl:relcl         10.42    5.21    6.94\n",
      "compound:smixut   53.19    8.71   14.97\n",
      "dep               28.57    1.87    3.51\n",
      "fixed             29.41    7.58   12.05\n",
      "appos              4.26    6.67    5.19\n",
      "nummod            95.16   52.21   67.43\n",
      "cop               77.42   44.44   56.47\n",
      "parataxis          0.00    0.00    0.00\n",
      "advcl              0.00    0.00    0.00\n",
      "advmod            59.84   38.38   46.77\n",
      "ccomp             15.15   14.71   14.93\n",
      "xcomp             32.20   44.19   37.25\n",
      "acl               50.00    8.70   14.81\n",
      "cc                53.85    3.87    7.22\n",
      "conj              19.67    5.77    8.92\n",
      "csubj              0.00    0.00    0.00\n",
      "nsubj:cop         33.33    5.88   10.00\n",
      "compound:affix     0.00    0.00    0.00\n",
      "aux               62.50   38.46   47.62\n",
      "\n",
      "\u001b[38;5;2mâœ” Saved results to metrics/urban-giggle.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model \n",
    "!python -m spacy project run evaluate /srv/projects/course-materials/w2/using-inception-data/newlang_project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8506562",
   "metadata": {},
   "source": [
    "### Package\n",
    "\n",
    "Finally, the **package** command saves your trained model in a single tar file that can be shared and installed on other computers.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e1d6f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mâ„¹ Building package artifacts: sdist\u001b[0m\n",
      "\u001b[38;5;2mâœ” Loaded meta.json from file\u001b[0m\n",
      "newlang_project/training/urban-giggle/model-last/meta.json\n",
      "\u001b[38;5;2mâœ” Generated README.md from meta.json\u001b[0m\n",
      "\u001b[38;5;2mâœ” Successfully created package 'yi_pipeline-0.0.0'\u001b[0m\n",
      "export/yi_pipeline-0.0.0\n",
      "running sdist\n",
      "running egg_info\n",
      "creating yi_pipeline.egg-info\n",
      "writing yi_pipeline.egg-info/PKG-INFO\n",
      "writing dependency_links to yi_pipeline.egg-info/dependency_links.txt\n",
      "writing entry points to yi_pipeline.egg-info/entry_points.txt\n",
      "writing requirements to yi_pipeline.egg-info/requires.txt\n",
      "writing top-level names to yi_pipeline.egg-info/top_level.txt\n",
      "writing manifest file 'yi_pipeline.egg-info/SOURCES.txt'\n",
      "reading manifest file 'yi_pipeline.egg-info/SOURCES.txt'\n",
      "reading manifest template 'MANIFEST.in'\n",
      "warning: no files found matching 'LICENSE'\n",
      "warning: no files found matching 'LICENSES_SOURCES'\n",
      "writing manifest file 'yi_pipeline.egg-info/SOURCES.txt'\n",
      "running check\n",
      "warning: check: missing required meta-data: url\n",
      "\n",
      "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
      "\n",
      "creating yi_pipeline-0.0.0\n",
      "creating yi_pipeline-0.0.0/yi_pipeline\n",
      "creating yi_pipeline-0.0.0/yi_pipeline.egg-info\n",
      "creating yi_pipeline-0.0.0/yi_pipeline/yi_pipeline-0.0.0\n",
      "creating yi_pipeline-0.0.0/yi_pipeline/yi_pipeline-0.0.0/parser\n",
      "creating yi_pipeline-0.0.0/yi_pipeline/yi_pipeline-0.0.0/tagger\n",
      "creating yi_pipeline-0.0.0/yi_pipeline/yi_pipeline-0.0.0/tok2vec\n",
      "creating yi_pipeline-0.0.0/yi_pipeline/yi_pipeline-0.0.0/vocab\n",
      "copying files to yi_pipeline-0.0.0...\n",
      "copying MANIFEST.in -> yi_pipeline-0.0.0\n",
      "copying README.md -> yi_pipeline-0.0.0\n",
      "copying meta.json -> yi_pipeline-0.0.0\n",
      "copying setup.py -> yi_pipeline-0.0.0\n",
      "copying yi_pipeline/__init__.py -> yi_pipeline-0.0.0/yi_pipeline\n",
      "copying yi_pipeline/meta.json -> yi_pipeline-0.0.0/yi_pipeline\n",
      "copying yi_pipeline.egg-info/PKG-INFO -> yi_pipeline-0.0.0/yi_pipeline.egg-info\n",
      "copying yi_pipeline.egg-info/SOURCES.txt -> yi_pipeline-0.0.0/yi_pipeline.egg-info\n",
      "copying yi_pipeline.egg-info/dependency_links.txt -> yi_pipeline-0.0.0/yi_pipeline.egg-info\n",
      "copying yi_pipeline.egg-info/entry_points.txt -> yi_pipeline-0.0.0/yi_pipeline.egg-info\n",
      "copying yi_pipeline.egg-info/not-zip-safe -> yi_pipeline-0.0.0/yi_pipeline.egg-info\n",
      "copying yi_pipeline.egg-info/requires.txt -> yi_pipeline-0.0.0/yi_pipeline.egg-info\n",
      "copying yi_pipeline.egg-info/top_level.txt -> yi_pipeline-0.0.0/yi_pipeline.egg-info\n",
      "copying yi_pipeline/yi_pipeline-0.0.0/README.md -> yi_pipeline-0.0.0/yi_pipeline/yi_pipeline-0.0.0\n",
      "copying yi_pipeline/yi_pipeline-0.0.0/config.cfg -> yi_pipeline-0.0.0/yi_pipeline/yi_pipeline-0.0.0\n",
      "copying yi_pipeline/yi_pipeline-0.0.0/meta.json -> yi_pipeline-0.0.0/yi_pipeline/yi_pipeline-0.0.0\n",
      "copying yi_pipeline/yi_pipeline-0.0.0/tokenizer -> yi_pipeline-0.0.0/yi_pipeline/yi_pipeline-0.0.0\n",
      "copying yi_pipeline/yi_pipeline-0.0.0/parser/cfg -> yi_pipeline-0.0.0/yi_pipeline/yi_pipeline-0.0.0/parser\n",
      "copying yi_pipeline/yi_pipeline-0.0.0/parser/model -> yi_pipeline-0.0.0/yi_pipeline/yi_pipeline-0.0.0/parser\n",
      "copying yi_pipeline/yi_pipeline-0.0.0/parser/moves -> yi_pipeline-0.0.0/yi_pipeline/yi_pipeline-0.0.0/parser\n",
      "copying yi_pipeline/yi_pipeline-0.0.0/tagger/cfg -> yi_pipeline-0.0.0/yi_pipeline/yi_pipeline-0.0.0/tagger\n",
      "copying yi_pipeline/yi_pipeline-0.0.0/tagger/model -> yi_pipeline-0.0.0/yi_pipeline/yi_pipeline-0.0.0/tagger\n",
      "copying yi_pipeline/yi_pipeline-0.0.0/tok2vec/cfg -> yi_pipeline-0.0.0/yi_pipeline/yi_pipeline-0.0.0/tok2vec\n",
      "copying yi_pipeline/yi_pipeline-0.0.0/tok2vec/model -> yi_pipeline-0.0.0/yi_pipeline/yi_pipeline-0.0.0/tok2vec\n",
      "copying yi_pipeline/yi_pipeline-0.0.0/vocab/key2row -> yi_pipeline-0.0.0/yi_pipeline/yi_pipeline-0.0.0/vocab\n",
      "copying yi_pipeline/yi_pipeline-0.0.0/vocab/lookups.bin -> yi_pipeline-0.0.0/yi_pipeline/yi_pipeline-0.0.0/vocab\n",
      "copying yi_pipeline/yi_pipeline-0.0.0/vocab/strings.json -> yi_pipeline-0.0.0/yi_pipeline/yi_pipeline-0.0.0/vocab\n",
      "copying yi_pipeline/yi_pipeline-0.0.0/vocab/vectors -> yi_pipeline-0.0.0/yi_pipeline/yi_pipeline-0.0.0/vocab\n",
      "Writing yi_pipeline-0.0.0/setup.cfg\n",
      "creating dist\n",
      "Creating tar archive\n",
      "removing 'yi_pipeline-0.0.0' (and everything under it)\n",
      "\u001b[38;5;2mâœ” Successfully created zipped Python package\u001b[0m\n",
      "export/yi_pipeline-0.0.0/dist/yi_pipeline-0.0.0.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy package ./newlang_project/training/urban-giggle/model-last ./export "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2dcda5",
   "metadata": {},
   "source": [
    "Keep in mind that you'll need some persistence and patience along the way. You'll probably need to run multiple experiments before you find the right blend of data and parameters to create a final product.  The instructors are happy to help along the way and we look forward to learning together with you. When all commands are successfully run, you will have converted your text annotations from inception and language object from Cadet into a trained statistical language model that can be loaded with spaCy for a large variety of research tasks.    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
