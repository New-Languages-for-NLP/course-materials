{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5be942ff",
   "metadata": {},
   "source": [
    "### Outline\n",
    "#### Goal is to train spaCy NER from litbank data \n",
    "\n",
    "- Load annotation data from LitBank\n",
    "- Create train and validation sets\n",
    "- Identify entities in text using Matcher (note missed ents in val set, not learning, just matching)\n",
    "- Train NER from scratch using only language object\n",
    "- Train NER from scratch  for small en model  \n",
    "- Fine-tune existing NER pipeline\n",
    "- Assess results for various approaches \n",
    "- Where do we see improvement? When is the model sufficiently useful in research?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3812c646",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy sklearn tqdm\n",
    "!python3 -m spacy download en_core_web_sm\n",
    "!git clone https://github.com/dbamman/litbank.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69950e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "entities_path = Path.cwd() / 'litbank' / 'entities' / 'brat'\n",
    "\n",
    "text_files = [f for f in entities_path.iterdir() if f.suffix == '.txt']\n",
    "assert len(text_files) == 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81333da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each file, create a Doc object and add the annotation data to doc.ents\n",
    "# our output is a list of Doc objects \n",
    "import spacy \n",
    "from tqdm.notebook import tqdm\n",
    "from spacy.tokens import Span, DocBin\n",
    "from spacy.util import filter_spans\n",
    "\n",
    "\n",
    "docs = []\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#TODO if using pretrained model, it adds predictions, need EntityRecognizer instead\n",
    "\n",
    "for text_file in tqdm(text_files):\n",
    "    doc = nlp(text_file.read_text())\n",
    "    annotation_file = (entities_path / (text_file.stem +'.ann'))\n",
    "    annotations = annotation_file.read_text().split('\\n')\n",
    "    lit_ents = []\n",
    "    for annotation in annotations[:-1]:\n",
    "        label, start, end = annotation.split('\\t')[1].split()\n",
    "        span = doc.char_span(int(start), int(end), label=label)\n",
    "        lit_ents.append(span)\n",
    "        # when start and end do not match a valid string, spaCy returns a NoneType span\n",
    "        lit_ents = [e for e in lit_ents if e] # remove NoneType spans from lit_ents\n",
    "        filtered = filter_spans(lit_ents)\n",
    "        doc.set_ents(filtered)\n",
    "    docs.append(doc)\n",
    "    \n",
    "assert len(docs) == 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc30f519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Docs into sets for training and validation \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_docs, validation_docs = train_test_split(docs, test_size=0.2)\n",
    "print(f'Created {len(train_docs)} training docs')\n",
    "print(f'Created {len(validation_docs)} validation docs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9830875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write Docs to disk as serialized binary files\n",
    "from spacy.tokens import DocBin\n",
    "test_docs = DocBin()\n",
    "for tdoc in train_docs: \n",
    "    test_docs.add(tdoc)\n",
    "Path('test_data.spacy').write_bytes(test_docs.to_bytes())\n",
    "\n",
    "val_docs = DocBin()\n",
    "for vdoc in validation_docs: \n",
    "    val_docs.add(vdoc)\n",
    "Path('validation_data.spacy').write_bytes(val_docs.to_bytes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d679c3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -al test_data.spacy validation_data.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedd73e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m spacy init config ./config.cfg --lang en --pipeline ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f89c589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the new config.cfg file \n",
    "!cat config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4d375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m spacy train ./config.cfg --output ./output --paths.train train.spacy --paths.dev dev.spacy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
