{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "061ba15a",
   "metadata": {},
   "source": [
    "This notebook: \n",
    "- Loads project file from GitHub\n",
    "- Loads assets from GitHub repo\n",
    "- installs the custom language object \n",
    "- converts the training data to spaCy binary\n",
    "- configure the project.yml file \n",
    "- train the model \n",
    "- assess performance \n",
    "- package the model (or push to huggingface) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0ba9e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp to clear project folder\n",
    "!rm -rf /srv/projects/course-materials/w2/using-inception-data/newlang_project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c0bda8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'old-chinese'...\n",
      "remote: Enumerating objects: 36, done.\u001b[K\n",
      "remote: Counting objects: 100% (36/36), done.\u001b[K\n",
      "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
      "remote: Total 36 (delta 9), reused 24 (delta 4), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (36/36), 8.30 KiB | 99.00 KiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "private_repo = True #@param {type:\"boolean\"}\n",
    "repo_name = \"old-chinese\" #@param {type:\"string\"}\n",
    "\n",
    "!rm -rf /content/newlang_project\n",
    "!rm -rf $repo_name\n",
    "if private_repo:\n",
    "    git_access_token = \"\" #@param {type:\"string\"}\n",
    "    git_url = f\"https://{git_access_token}@github.com/New-Languages-for-NLP/{repo_name}/\"\n",
    "    !git clone $git_url  -b main\n",
    "    !cp -r ./$repo_name/newlang_project .  \n",
    "    !mkdir newlang_project/assets/\n",
    "    !mkdir newlang_project/configs/\n",
    "    !mkdir newlang_project/corpus/\n",
    "    !mkdir newlang_project/metrics/\n",
    "    !mkdir newlang_project/packages/\n",
    "    !mkdir newlang_project/training/\n",
    "    !mkdir newlang_project/assets/$repo_name\n",
    "    !cp -r ./$repo_name/* newlang_project/assets/$repo_name/\n",
    "    !rm -rf ./$repo_name\n",
    "else:\n",
    "    !python -m spacy project clone newlang_project --repo https://github.com/New-Languages-for-NLP/$repo_name --branch main\n",
    "    !python -m spacy project assets /content/newlang_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dc13741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: python: command not found\r\n"
     ]
    }
   ],
   "source": [
    "# Install the custom language object from Cadet \n",
    "!python -m spacy project run install /srv/projects/course-materials/w2/using-inception-data/newlang_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "be6828d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "=================================== config ===================================\u001b[0m\n",
      "Running command: /srv/projects/course-materials/temp/venv/bin/python -m spacy init config config.cfg --lang clara -F\n",
      "\u001b[38;5;3mâš  To generate a more effective transformer-based config (GPU-only),\n",
      "install the spacy-transformers package and re-run this command. The config\n",
      "generated now does not use transformers.\u001b[0m\n",
      "\u001b[38;5;4mâ„¹ Generated config template specific for your use case\u001b[0m\n",
      "- Language: clara\n",
      "- Pipeline: tagger, parser, ner\n",
      "- Optimize for: efficiency\n",
      "- Hardware: CPU\n",
      "- Transformer: None\n",
      "\u001b[38;5;2mâœ” Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2mâœ” Saved config\u001b[0m\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "# Create training config\n",
    "!python -m spacy project run config /srv/projects/course-materials/w2/using-inception-data/newlang_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "563fdc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "================================== convert ==================================\u001b[0m\n",
      "Running command: /srv/projects/course-materials/temp/venv/bin/python scripts/convert.py assets/urban-giggle/3_inception_export\n",
      "\u001b[38;5;4mâ„¹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2mâœ” Generated output file (106 documents): corpus/YorText2.spacy\u001b[0m\n",
      "\u001b[38;5;4mâ„¹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2mâœ” Generated output file (360 documents): corpus/YorText3.spacy\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Convert the conllu files from inception to spaCy binary format\n",
    "# Currently requires edit to spacy/training/converters/conllu_to_docs.py line 194 \n",
    "# if pos == \"_\":                                                                                                                  \n",
    "#     pos = \"\"\n",
    "\n",
    "!python -m spacy project run convert /srv/projects/course-materials/w2/using-inception-data/newlang_project -F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9519c858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "=================================== split ===================================\u001b[0m\n",
      "Running command: /srv/projects/course-materials/temp/venv/bin/python scripts/split.py 0.4 11\n",
      "ðŸ˜Š Created 279 training docs\n",
      "ðŸ˜Š Created 187 validation docs\n"
     ]
    }
   ],
   "source": [
    "# Read data files, convert to spaCy files\n",
    "# test/train split \n",
    "!python -m spacy project run split /srv/projects/course-materials/w2/using-inception-data/newlang_project -F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4feefe6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "=================================== debug ===================================\u001b[0m\n",
      "Running command: /srv/projects/course-materials/temp/venv/bin/python scripts/update_config.py\n",
      "Running command: /srv/projects/course-materials/temp/venv/bin/python -m spacy debug data ./config.cfg\n",
      "\u001b[1m\n",
      "============================ Data file validation ============================\u001b[0m\n",
      "\u001b[38;5;2mâœ” Pipeline can be initialized with data\u001b[0m\n",
      "\u001b[38;5;2mâœ” Corpus is loadable\u001b[0m\n",
      "\u001b[1m\n",
      "=============================== Training stats ===============================\u001b[0m\n",
      "Language: clara\n",
      "Training pipeline: tok2vec, tagger, parser, ner\n",
      "279 training docs\n",
      "187 evaluation docs\n",
      "\u001b[38;5;2mâœ” No overlap between training and evaluation data\u001b[0m\n",
      "\u001b[38;5;3mâš  Low number of examples to train a new pipeline (279)\u001b[0m\n",
      "\u001b[1m\n",
      "============================== Vocab & Vectors ==============================\u001b[0m\n",
      "\u001b[38;5;4mâ„¹ 38701 total word(s) in the data (5648 unique)\u001b[0m\n",
      "\u001b[38;5;4mâ„¹ No word vectors present in the package\u001b[0m\n",
      "\u001b[1m\n",
      "========================== Named Entity Recognition ==========================\u001b[0m\n",
      "\u001b[38;5;4mâ„¹ 0 label(s)\u001b[0m\n",
      "0 missing value(s) (tokens with '-' label)\n",
      "\u001b[38;5;2mâœ” Good amount of examples for all labels\u001b[0m\n",
      "\u001b[38;5;2mâœ” Examples without occurrences available for all labels\u001b[0m\n",
      "\u001b[38;5;2mâœ” No entities consisting of or starting/ending with whitespace\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Part-of-speech Tagging ===========================\u001b[0m\n",
      "\u001b[38;5;4mâ„¹ 1 label(s) in train data\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Dependency Parsing =============================\u001b[0m\n",
      "\u001b[38;5;4mâ„¹ Found 38691 sentence(s) with an average length of 1.0 words.\u001b[0m\n",
      "\u001b[38;5;4mâ„¹ 1 label(s) in train data\u001b[0m\n",
      "\u001b[38;5;4mâ„¹ 1 label(s) in projectivized train data\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Summary ==================================\u001b[0m\n",
      "\u001b[38;5;2mâœ” 6 checks passed\u001b[0m\n",
      "\u001b[38;5;3mâš  1 warning\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Debug the data\n",
    "!python -m spacy project run debug  /srv/projects/course-materials/w2/using-inception-data/newlang_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8d32abf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "=================================== train ===================================\u001b[0m\n",
      "Running command: /srv/projects/course-materials/temp/venv/bin/python -m spacy train config.cfg --output training/urban-giggle --gpu-id -1 --nlp.lang=clara\n",
      "\u001b[38;5;2mâœ” Created output directory: training/urban-giggle\u001b[0m\n",
      "\u001b[38;5;4mâ„¹ Saving to output directory: training/urban-giggle\u001b[0m\n",
      "\u001b[38;5;4mâ„¹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2021-12-19 20:17:02,411] [INFO] Set up nlp object from config\n",
      "[2021-12-19 20:17:02,416] [INFO] Pipeline: ['tok2vec', 'tagger', 'parser', 'ner']\n",
      "[2021-12-19 20:17:02,419] [INFO] Created vocabulary\n",
      "[2021-12-19 20:17:02,419] [INFO] Finished initializing nlp object\n",
      "[2021-12-19 20:17:04,527] [INFO] Initialized pipeline components: ['tok2vec', 'tagger', 'parser', 'ner']\n",
      "\u001b[38;5;2mâœ” Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mâ„¹ Pipeline: ['tok2vec', 'tagger', 'parser', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mâ„¹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS TAGGER  LOSS PARSER  LOSS NER  TAG_ACC  DEP_UAS  DEP_LAS  SENTS_F  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  -----------  -----------  --------  -------  -------  -------  -------  ------  ------  ------  ------\n",
      "('L-dep', 0, 9000.0)\n",
      "('R-dep', 0, 9000.0)\n",
      "('B-_', 0, 9000.0)\n",
      "('B-ROOT', 0, 9000.0)\n",
      "('Gold sent starts?', 1, 1)\n",
      "  0       0          0.00         0.00         0.00    107.00   100.00   100.00     0.00   100.00    0.00    0.00    0.00    0.50\n",
      "('L-dep', 0, 9000.0)\n",
      "('R-dep', 0, 9000.0)\n",
      "('B-_', 0, 9000.0)\n",
      "('B-ROOT', 0, 9000.0)\n",
      "('Gold sent starts?', 1, 1)\n",
      "\u001b[38;5;3mâš  Aborting and saving the final best model. Encountered exception:\n",
      "ValueError('Could not find gold transition - see logs above.')\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/srv/projects/course-materials/temp/venv/lib/python3.8/site-packages/spacy/__main__.py\", line 4, in <module>\n",
      "    setup_cli()\n",
      "  File \"/srv/projects/course-materials/temp/venv/lib/python3.8/site-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
      "    command(prog_name=COMMAND)\n",
      "  File \"/srv/projects/course-materials/temp/venv/lib/python3.8/site-packages/click/core.py\", line 1128, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/srv/projects/course-materials/temp/venv/lib/python3.8/site-packages/click/core.py\", line 1053, in main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/srv/projects/course-materials/temp/venv/lib/python3.8/site-packages/click/core.py\", line 1659, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"/srv/projects/course-materials/temp/venv/lib/python3.8/site-packages/click/core.py\", line 1395, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/srv/projects/course-materials/temp/venv/lib/python3.8/site-packages/click/core.py\", line 754, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"/srv/projects/course-materials/temp/venv/lib/python3.8/site-packages/typer/main.py\", line 500, in wrapper\n",
      "    return callback(**use_params)  # type: ignore\n",
      "  File \"/srv/projects/course-materials/temp/venv/lib/python3.8/site-packages/spacy/cli/train.py\", line 45, in train_cli\n",
      "    train(config_path, output_path, use_gpu=use_gpu, overrides=overrides)\n",
      "  File \"/srv/projects/course-materials/temp/venv/lib/python3.8/site-packages/spacy/cli/train.py\", line 75, in train\n",
      "    train_nlp(nlp, output_path, use_gpu=use_gpu, stdout=sys.stdout, stderr=sys.stderr)\n",
      "  File \"/srv/projects/course-materials/temp/venv/lib/python3.8/site-packages/spacy/training/loop.py\", line 122, in train\n",
      "    raise e\n",
      "  File \"/srv/projects/course-materials/temp/venv/lib/python3.8/site-packages/spacy/training/loop.py\", line 105, in train\n",
      "    for batch, info, is_best_checkpoint in training_step_iterator:\n",
      "  File \"/srv/projects/course-materials/temp/venv/lib/python3.8/site-packages/spacy/training/loop.py\", line 203, in train_while_improving\n",
      "    nlp.update(\n",
      "  File \"/srv/projects/course-materials/temp/venv/lib/python3.8/site-packages/spacy/language.py\", line 1153, in update\n",
      "    proc.update(examples, sgd=None, losses=losses, **component_cfg[name])  # type: ignore\n",
      "  File \"spacy/pipeline/transition_parser.pyx\", line 406, in spacy.pipeline.transition_parser.Parser.update\n",
      "  File \"spacy/pipeline/transition_parser.pyx\", line 518, in spacy.pipeline.transition_parser.Parser.get_batch_loss\n",
      "  File \"spacy/pipeline/_parser_internals/arc_eager.pyx\", line 827, in spacy.pipeline._parser_internals.arc_eager.ArcEager.set_costs\n",
      "ValueError: Could not find gold transition - see logs above.\n"
     ]
    }
   ],
   "source": [
    "# Train the model \n",
    "!python -m spacy project run train /srv/projects/course-materials/w2/using-inception-data/newlang_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018362d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model \n",
    "!python -m spacy project run evaluate /srv/projects/course-materials/w2/using-inception-data/newlang_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e1d6f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: python: command not found\r\n"
     ]
    }
   ],
   "source": [
    "# Package the model \n",
    "!mkdir ./export \n",
    "!python -m spacy package ./newlang_project/training/urban-giggle/model-last ./export "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97783aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
