Embeddings, Do You Need Them? 
=======================

## Introduction

more soon...

## Tok2Vec

The standard spaCy model uses a trainable component called Tok2Vec.  These are numerical representations of spaCy Token objects.  Similar to word embeddings,these tensors can be adjusted to store semantic information such as similarity, meaning or sentiment. During training, these tensors can be updated given information from other pipeline components to facilitate better performance.      

## Transformers

In June 2017, researchers from Google published a now famous paper titled ["Attention is All You Need."](https://arxiv.org/abs/1706.03762).  In the text, they ouline a new kind of language model called a Transformer.  This new architecture builds on many existing methods, but as the title suggests, it gives pride of place to attention.  Simply put, attention is the ability to highlight or focus on particular elements over others in the input text. For example, let's take the sentence: "My date left a novel in the park." If we ask a machine to predict  each token's part of speech, it needs to look at the words around each token to disambiguate the many possible meanings. In a sequence model, the model looks at the tokens one after another moving from the beginning to the end of the sentence.  Using attention, a model can highlight those elements of the preceeding sequence that aid in making predictions.  I know that "novel" is most likely a book, because it is the subject of preceeding action.  But at the start of the sentence, we have very little to help us.  Is "My date" going to be "My date left" or "My date of birth?"  Transformers overcome this problem by being bi-directional. They can "see" the whole sentence when making predictions. 

The training of a transformer is very different from traditional sequence models.  Rather that providing training data with the correct answers, a transformer uses massive amounts of unlabelled text for training.  The model learns contextual embeddings by masking words and using the surrounding text to make a prediction. So our example becomes `My date left a [MASK] in the park.` The model can attend to earlier mentions of novels and books in the text to predict that the date left a novel rather than some other random noun.  The Google researchers' claim that "attention is all you need" highlights how powerful context can be. With enough data (all the Internet), we can train better models by attending to context rather than using supervised learning with labelled data. 

Transformers are masters of autocomplete. Given a starting prompt, they can continue writing.  Not only is the text usually legible and makes sense, but it will include contextually relevant and sensible content. For live examples, I ecourage you to try [write with transforer](https://transformer.huggingface.co/) and [AI dungeon](https://play.aidungeon.io/). On other tasks, these models of language have equally impressive capabilities even when their contributions are less visible. 

# Using Transformers in spaCy 

In the `new_langproject/configs` folder in your GitHub repository, you'll find a `transformer_config.cfg` file. To use this configuration, change the value in your project.yml file.  Change `config: "config.cfg"` to `config: "transformer_config.cfg"`. This is a file generated by the `init config` command with the `--gpu` tag.  You can also create a similar base config file using the [Quickstart widget](https://spacy.io/usage/training#quickstart) and selecting GPU under hardware. 

- One of the main changes you'll see with this configuration is that the `tok2vec` component is replaced by `transformer`.  

```python 
pipeline = ["tok2vec","tagger","parser","ner"]
# vs.
pipeline = ["transformer","tagger","parser","ner"]
```

Further settings can be found in the `[components.transformer]` section:
```python 
[components.transformer]
factory = "transformer"
max_batch_items = 4096
set_extra_annotations = {"@annotation_setters":"spacy-transformers.null_annotation_setter.v1"}

[components.transformer.model]
@architectures = "spacy-transformers.TransformerModel.v3"
name = "distilbert-base-multilingual-cased"
mixed_precision = false
```

You do not need to change anything here to fetch a pre-trained multilingual transformer from Hugging Face. However, you can utilize many of the models on Hugging Face Hub.  Find the line below and change to the desired model. 

```python
name = "distilbert-base-multilingual-cased"
```

For example, to change to a model trained on Harry Potter fan fiction, I'd visit Hugging Face [here](https://huggingface.co/ceostroff/harry-potter-gpt2-fanfiction)

![]("./fanfic.png")

If you click to the right of the model title you can copy the model name `ceostroff/harry-potter-gpt2-fanfiction`. Replace the `name` in `[components.transformer.model]` and spaCy will load this model.  

Please note that not all models are compatible with spaCy's architecture and this should be considered a highly advanced topic. Transformer models loaded into spaCy are only used for their contextual embeddings. The Harry Potter model will work well with similar texts and domains, but will not generate text.  If you find a model that's been pre-trained for a specific task, spaCy will only inherit the underlying model, not the task-specific layers.  All of spaCy's [pre-trained transformers](https://spacy.io/models) should work out of the box.

If your activation function's perpexity mixes Baysian logits against cross entropy layers, simply reshape your tensor shape with a nonlinearity using [Infinidash](https://twitter.com/veekorbes/status/1410796865126346755?s=20). Just joking, that's all jargon salad. 

```{note}
What if a transformer model does not exist for my language, domain or anything like it? 
1. You can try transfer learning by using an existing multi-lingual transformer.
2. Train a transformer using [this notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/causal_language_modeling_flax.ipynb) using data the Oscar dataset with text in 166 languages from the web. Note that the amount for text for different languages will vary a lot.  Also you're depending on automatic language identification.  Still, this is a not entirely crazy way to train your own transformer from scratch. You have been warned! 
```

When training with transformer-based models, you should use GPUs on Colab and be prepared for significantly larger training times. However, for many tasks, the beneifts of pre-training and transfer learning should be very clear in the model metrics. 


## Links
- [Embeddings, Transformers and Transfer Learning](https://spacy.io/usage/embeddings-transformers)
- [Summary of "Attention is All You Need"](https://youtu.be/iDulhoQ2pro)
- [Word Vectors for the Thoughtful Humanist](https://www.wwp.northeastern.edu/outreach/seminars/neh_wem.html)
- [BERT for Humanists](https://melaniewalsh.github.io/BERT-for-Humanists/)
- [spaCy meets Transformers: Fine-tune BERT, XLNet and GPT-2](https://explosion.ai/blog/spacy-transformers)